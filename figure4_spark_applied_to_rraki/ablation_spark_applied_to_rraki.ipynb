{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I will try applying SPARK to residual RAKI for a variety of acceleration rates and ACS sizes, to see which combinations of ACS size + acceleration would be best to show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/yarefeen/anaconda3/envs/tf-gpu/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "#rraki imports\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import numpy.matlib\n",
    "import time\n",
    "import os\n",
    "\n",
    "from utils import signalprocessing as sig\n",
    "\n",
    "#additional spark imports which may be needed\n",
    "import importlib \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import warnings\n",
    "import numpy.linalg as la\n",
    "import cupy as cp\n",
    "import matplotlib.pyplot as plt\n",
    "from bart import bart\n",
    "from utils import cfl\n",
    "from utils import signalprocessing as sig\n",
    "from utils import models\n",
    "from utils import iterative\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### defining rraki helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape,vari_name):                   \n",
    "    initial = tf.truncated_normal(shape, stddev=0.1,dtype=tf.float32)\n",
    "    return tf.Variable(initial,name = vari_name)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape,dtype=tf.float32)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d_same(x,W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def conv2d_dilate_same(x, W,dilate_rate):\n",
    "    return tf.nn.convolution(x, W,padding='SAME',dilation_rate = [1,dilate_rate])\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='VALID')\n",
    "\n",
    "def conv2d_dilate(x, W,dilate_rate):\n",
    "    return tf.nn.convolution(x, W,padding='VALID',dilation_rate = [1,dilate_rate])\n",
    "\n",
    "def learning_residual_raki(ACS_input,target_input,accrate_input,sess,\\\n",
    "    ACS_dim_X,ACS_dim_Y,ACS_dim_Z,target_dim_X,target_dim_Y,target_dim_Z,\\\n",
    "    target,target_x_start,target_x_end,target_y_start,target_y_end,\\\n",
    "    ACS):\n",
    "    input_ACS    = tf.placeholder(tf.float32, [1, ACS_dim_X,ACS_dim_Y,ACS_dim_Z])                                  \n",
    "    input_Target = tf.placeholder(tf.float32, [1, target_dim_X,target_dim_Y,target_dim_Z]) \n",
    "    \n",
    "    Input = tf.reshape(input_ACS, [1, ACS_dim_X, ACS_dim_Y, ACS_dim_Z])         \n",
    "\n",
    "    [target_dim0,target_dim1,target_dim2,target_dim3] = np.shape(target)\n",
    "    \n",
    "    W_conv1 = weight_variable([kernel_x_1, kernel_y_1, ACS_dim_Z, layer1_channels],'W1') \n",
    "    h_conv1 = tf.nn.relu(conv2d_dilate(Input, W_conv1,accrate_input)) \n",
    "\n",
    "    W_conv2 = weight_variable([kernel_x_2, kernel_y_2, layer1_channels, layer2_channels],'W2')\n",
    "    h_conv2 = tf.nn.relu(conv2d_dilate(h_conv1, W_conv2,accrate_input))\n",
    "\n",
    "    W_conv3 = weight_variable([kernel_last_x, kernel_last_y, layer2_channels, target_dim3],'W3')\n",
    "    h_conv3 = conv2d_dilate(h_conv2, W_conv3,accrate_input)\n",
    "    \n",
    "    W_conv_linear = weight_variable([kernel_x_linear,kernel_y_linear,ACS_dim_Z,target_dim3],'W_lin')\n",
    "    h_linear = conv2d_dilate(Input,W_conv_linear,accrate_input)\n",
    "    x_length = h_conv3.shape[1]\n",
    "    y_length = h_conv3.shape[2]\n",
    "    \n",
    "    if(y_length % 2 == 0):\n",
    "        h_linear = h_linear[:,x_length//2 - x_length//2:x_length//2 + x_length//2,\\\n",
    "            y_length//2 - y_length//2:y_length//2 + y_length//2,:]\n",
    "    else:\n",
    "        h_linear = h_linear[:,x_length//2 - x_length//2:x_length//2 + x_length//2,\\\n",
    "            y_length//2 - y_length//2:y_length//2 + y_length//2+1,:]\n",
    "        \n",
    "    error_norm = tf.norm(input_Target - h_linear) + tf.norm(input_Target - h_linear - h_conv3)\n",
    "    train_step = tf.train.AdamOptimizer(LearningRate).minimize(error_norm)\n",
    "    \n",
    "     \n",
    "    if int((tf.__version__).split('.')[1]) < 12 and int((tf.__version__).split('.')[0]) < 1:\n",
    "        init = tf.initialize_all_variables()\n",
    "    else:\n",
    "        init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    error_prev = 1 \n",
    "    for i in range(MaxIteration):\n",
    "        \n",
    "        sess.run(train_step, feed_dict={input_ACS: ACS, input_Target: target})\n",
    "        if i % 100 == 0:                                                                      \n",
    "            error_now=sess.run(error_norm,feed_dict={input_ACS: ACS, input_Target: target})    \n",
    "            print('The',i,'th iteration gives an error',error_now)  \n",
    "            \n",
    "    error = sess.run(error_norm,feed_dict={input_ACS: ACS, input_Target: target})\n",
    "    return [sess.run(W_conv1),sess.run(W_conv2),sess.run(W_conv3),sess.run(W_conv_linear),error]  \n",
    "    \n",
    "def cnn_linear(input_kspace,w_linear,acc_rate,sess):\n",
    "    return sess.run(conv2d_dilate(input_kspace,w_linear,acc_rate))\n",
    "\n",
    "def cnn_3layer(input_kspace,w1,w2,w3,acc_rate,sess):                \n",
    "    h_conv1 = tf.nn.relu(conv2d_dilate(input_kspace, w1,acc_rate)) \n",
    "    h_conv2 = tf.nn.relu(conv2d_dilate(h_conv1, w2,acc_rate))\n",
    "    h_conv3 = conv2d_dilate(h_conv2, w3,acc_rate) \n",
    "    return sess.run(h_conv3)           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading k-space and defining fft operators "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fft2c_raki  = lambda x: sig.fft(sig.fft(x,0),1)\n",
    "ifft2c_raki = lambda x: sig.ifft(sig.ifft(x,0),1)\n",
    "\n",
    "image_coils_truth  = sio.loadmat('data/img_grappa_32chan.mat')['IMG']\n",
    "\n",
    "kspace_truth_raki  = fft2c_raki(image_coils_truth)\n",
    "[M,N,C] = kspace_truth_raki.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### setting non-changing residual-raki parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_FRAC = 1/4\n",
    "\n",
    "Rx     = 1\n",
    "acsx   = M\n",
    "\n",
    "#### Linear Network Parameters ####\n",
    "kernel_x_linear = 5\n",
    "kernel_y_linear = 2\n",
    "\n",
    "#### RAKI Network Parameters ####\n",
    "kernel_x_1 = 5\n",
    "kernel_y_1 = 2\n",
    "\n",
    "kernel_x_2 = 1\n",
    "kernel_y_2 = 1\n",
    "\n",
    "kernel_last_x = 3\n",
    "kernel_last_y = 2\n",
    "\n",
    "layer1_channels = 32 \n",
    "layer2_channels = 8\n",
    "\n",
    "MaxIteration = 1000\n",
    "LearningRate = 3e-3  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### setting ablation parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_Ry    = [5,6]\n",
    "all_acsy  = [20,24,30,36,40]\n",
    "             \n",
    "all_parameters = []\n",
    "\n",
    "for ry in all_Ry:\n",
    "    for acsy in all_acsy:\n",
    "        all_parameters.append({'Ry':ry, 'acsy':acsy})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### designing residual raki-reconstruction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_raki(MaxIteration,LearningRate,Rx,Ry,acsx,acsy,GPU_FRAC,\\\n",
    "    kspace_truth_raki=kspace_truth_raki,kernel_x_linear=kernel_x_linear,\\\n",
    "    kernel_y_linear=kernel_y_linear,kernel_x_1=kernel_x_1,kernel_y_1=kernel_y_1,kernel_x_2=kernel_x_2,\\\n",
    "    kernel_y_2=kernel_y_2,kernel_last_x=kernel_last_x,kernel_last_y=kernel_last_y,\n",
    "    layer1_channels=layer1_channels,layer2_channels=layer2_channels):\n",
    "    \n",
    "    acsregionX = np.arange(M//2 - acsx // 2,M//2 + acsx//2) \n",
    "    acsregionY = np.arange(N//2 - acsy // 2,N//2 + acsy//2) \n",
    "\n",
    "    kspace_raki_undersampled_withacs = np.zeros((M,N,C),dtype = complex)\n",
    "    kspace_raki_undersampled_withacs[::Rx,::Ry,:] = kspace_truth_raki[::Rx,::Ry,:]\n",
    "    kspace_raki_undersampled_withacs[acsregionX[0]:acsregionX[acsx-1]+1,acsregionY[0]:acsregionY[acsy-1]+1,:]\\\n",
    "        = kspace_truth_raki[acsregionX[0]:acsregionX[acsx-1]+1,acsregionY[0]:acsregionY[acsy-1]+1,:]\n",
    "\n",
    "    kspace = np.copy(kspace_raki_undersampled_withacs)\n",
    "    no_ACS_flag = 0;\n",
    "    normalize = 0.015/np.max(abs(kspace[:]))\n",
    "    kspace = np.multiply(kspace,normalize) \n",
    "    cur_kspace_truth_raki = np.multiply(kspace_truth_raki,normalize)\n",
    "    \n",
    "    [m1,n1,no_ch] = np.shape(kspace)\n",
    "    no_inds = 1\n",
    "\n",
    "    kspace_all = np.copy(kspace);\n",
    "    kx = np.transpose(np.int32([(range(1,m1+1))]))                  \n",
    "    ky = np.int32([(range(1,n1+1))])\n",
    "\n",
    "    kspace = np.copy(kspace_all)\n",
    "    mask = np.squeeze(np.matlib.sum(np.matlib.sum(np.abs(kspace),0),1))>0; \n",
    "    picks = np.where(mask == 1);                                  \n",
    "    kspace = kspace[:,np.int32(picks[0][0]):n1+1,:]\n",
    "    kspace_all = kspace_all[:,np.int32(picks[0][0]):n1+1,:]  \n",
    "\n",
    "    kspace_NEVER_TOUCH = np.copy(kspace_all)\n",
    "\n",
    "    mask = np.squeeze(np.matlib.sum(np.matlib.sum(np.abs(kspace),0),1))>0;  \n",
    "    picks = np.where(mask == 1);                                  \n",
    "    d_picks = np.diff(picks,1)  \n",
    "    indic = np.where(d_picks == 1);\n",
    "\n",
    "    mask_x = np.squeeze(np.matlib.sum(np.matlib.sum(np.abs(kspace),2),1))>0;\n",
    "    picks_x = np.where(mask_x == 1);\n",
    "    x_start = picks_x[0][0]\n",
    "    x_end = picks_x[0][-1]\n",
    "\n",
    "\n",
    "    no_ACS_flag=0;\n",
    "    print('ACS signal found in the input data')\n",
    "    indic = indic[1][:]\n",
    "    center_start = picks[0][indic[0]];\n",
    "    center_end = picks[0][indic[-1]+1];\n",
    "    ACS = kspace[x_start:x_end+1,center_start:center_end+1,:]\n",
    "    [ACS_dim_X, ACS_dim_Y, ACS_dim_Z] = np.shape(ACS)\n",
    "    ACS_re = np.zeros([ACS_dim_X,ACS_dim_Y,ACS_dim_Z*2])\n",
    "    ACS_re[:,:,0:no_ch] = np.real(ACS)\n",
    "    ACS_re[:,:,no_ch:no_ch*2] = np.imag(ACS)\n",
    "\n",
    "    acc_rate = d_picks[0][0]\n",
    "    no_channels = ACS_dim_Z*2\n",
    "\n",
    "    time_ALL_start = time.time()\n",
    "\n",
    "    [ACS_dim_X, ACS_dim_Y, ACS_dim_Z] = np.shape(ACS_re)\n",
    "    ACS = np.reshape(ACS_re, [1,ACS_dim_X, ACS_dim_Y, ACS_dim_Z]) \n",
    "    ACS = np.float32(ACS)  \n",
    "    \n",
    "    w_linear_all = \\\n",
    "        np.zeros([kernel_x_linear, kernel_y_linear, no_channels, acc_rate - 1, no_channels],dtype=np.float32)\n",
    "        \n",
    "    w1_all = np.zeros([kernel_x_1, kernel_y_1, no_channels, layer1_channels, no_channels],dtype=np.float32)\n",
    "    w2_all = np.zeros([kernel_x_2, kernel_y_2, layer1_channels,layer2_channels,no_channels],dtype=np.float32)\n",
    "    w3_all = np.zeros([kernel_last_x, kernel_last_y, layer2_channels,acc_rate - 1, no_channels],dtype=np.float32)    \n",
    "\n",
    "    target_x_start = np.int32(np.ceil(kernel_x_1/2) + np.floor(kernel_x_2/2) + np.floor(kernel_last_x/2) -1); \n",
    "    target_x_end = np.int32(ACS_dim_X - target_x_start -1); \n",
    "\n",
    "    target_y_start = np.int32((np.ceil(kernel_y_1/2)-1) + (np.ceil(kernel_y_2/2)-1) + (np.ceil(kernel_last_y/2)-1)) * acc_rate;     \n",
    "    target_y_end = ACS_dim_Y  - np.int32((np.floor(kernel_y_1/2) + np.floor(kernel_y_2/2) + np.floor(kernel_last_y/2))) * acc_rate -1;\n",
    "\n",
    "    target_dim_X = target_x_end - target_x_start + 1\n",
    "    target_dim_Y = target_y_end - target_y_start + 1\n",
    "    target_dim_Z = acc_rate - 1\n",
    "    \n",
    "    print('go!')\n",
    "    time_Learn_start = time.time() \n",
    "\n",
    "    errorSum = 0;\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.per_process_gpu_memory_fraction = GPU_FRAC ; \n",
    "\n",
    "    for ind_c in range(ACS_dim_Z):\n",
    "\n",
    "        sess = tf.Session(config=config)\n",
    "        # set target lines\n",
    "        target = np.zeros([1,target_dim_X,target_dim_Y,target_dim_Z])\n",
    "        print('learning channel #',ind_c+1)\n",
    "        time_channel_start = time.time()\n",
    "\n",
    "        for ind_acc in range(acc_rate-1):\n",
    "            target_y_start = np.int32((np.ceil(kernel_y_1/2)-1) + (np.ceil(kernel_y_2/2)-1) + (np.ceil(kernel_last_y/2)-1)) * acc_rate + ind_acc + 1 \n",
    "            target_y_end = ACS_dim_Y  - np.int32((np.floor(kernel_y_1/2) + (np.floor(kernel_y_2/2)) + np.floor(kernel_last_y/2))) * acc_rate + ind_acc\n",
    "            target[0,:,:,ind_acc] = ACS[0,target_x_start:target_x_end + 1, target_y_start:target_y_end +1,ind_c];\n",
    "\n",
    "        # learning\n",
    "\n",
    "        [w1,w2,w3,w_linear,error]=learning_residual_raki(ACS,target,acc_rate,sess,\\\n",
    "            ACS_dim_X,ACS_dim_Y,ACS_dim_Z,target_dim_X,target_dim_Y,target_dim_Z,target,\\\n",
    "            target_x_start,target_x_end,target_y_start,target_y_end,\\\n",
    "            ACS) \n",
    "        w1_all[:,:,:,:,ind_c] = w1\n",
    "        w2_all[:,:,:,:,ind_c] = w2\n",
    "        w3_all[:,:,:,:,ind_c] = w3     \n",
    "        w_linear_all[:,:,:,:,ind_c] = w_linear\n",
    "        \n",
    "        time_channel_end = time.time()\n",
    "        print('Time Cost:',time_channel_end-time_channel_start,'s')\n",
    "        print('Norm of Error = ',error)\n",
    "        errorSum = errorSum + error\n",
    "\n",
    "        sess.close()\n",
    "        tf.reset_default_graph()\n",
    "\n",
    "    time_Learn_end = time.time();\n",
    "\n",
    "    print('lerning step costs:',(time_Learn_end - time_Learn_start)/60,'min')    \n",
    "    kspace_recon_all = np.copy(kspace_all)\n",
    "    kspace_recon_all_nocenter = np.copy(kspace_all)\n",
    "\n",
    "    kspace = np.copy(kspace_all)\n",
    "\n",
    "    over_samp = np.setdiff1d(picks,np.int32([range(0, n1,acc_rate)]))\n",
    "    kspace_und = kspace\n",
    "    kspace_und[:,over_samp,:] = 0;\n",
    "    [dim_kspaceUnd_X,dim_kspaceUnd_Y,dim_kspaceUnd_Z] = np.shape(kspace_und)\n",
    "\n",
    "    kspace_und_re = np.zeros([dim_kspaceUnd_X,dim_kspaceUnd_Y,dim_kspaceUnd_Z*2])\n",
    "    kspace_und_re[:,:,0:dim_kspaceUnd_Z] = np.real(kspace_und)\n",
    "    kspace_und_re[:,:,dim_kspaceUnd_Z:dim_kspaceUnd_Z*2] = np.imag(kspace_und)\n",
    "    kspace_und_re = np.float32(kspace_und_re)\n",
    "    kspace_und_re = np.reshape(kspace_und_re,[1,dim_kspaceUnd_X,dim_kspaceUnd_Y,dim_kspaceUnd_Z*2])\n",
    "    kspace_recon = kspace_und_re\n",
    "\n",
    "    kspace_recon_linear       = np.copy(kspace_recon)\n",
    "    kspace_recon_residual     = np.zeros(kspace_recon.shape,dtype = complex)\n",
    "        \n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.per_process_gpu_memory_fraction = GPU_FRAC ; \n",
    "\n",
    "    for ind_c in range(0,no_channels):\n",
    "        print('Reconstruting Channel #',ind_c+1)\n",
    "\n",
    "        sess = tf.Session(config=config) \n",
    "        if int((tf.__version__).split('.')[1]) < 12 and int((tf.__version__).split('.')[0]) < 1:\n",
    "            init = tf.initialize_all_variables()\n",
    "        else:\n",
    "            init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "\n",
    "        # grab w and b\n",
    "        w1 = np.float32(w1_all[:,:,:,:,ind_c])\n",
    "        w2 = np.float32(w2_all[:,:,:,:,ind_c])     \n",
    "        w3 = np.float32(w3_all[:,:,:,:,ind_c])\n",
    "\n",
    "        w_linear = np.float32(w_linear_all[:,:,:,:,ind_c])\n",
    "        \n",
    "        residual_recon = cnn_3layer(kspace_und_re,w1,w2,w3,acc_rate,sess) \n",
    "        linear_recon   = cnn_linear(kspace_und_re,w_linear,acc_rate,sess)  \n",
    "        x_length = residual_recon.shape[1]\n",
    "        y_length = residual_recon.shape[2]\n",
    "        linear_recon = linear_recon[:,x_length//2 - x_length//2:x_length//2 + x_length//2,\\\n",
    "                y_length//2 - y_length//2:y_length//2 + y_length//2,:]\n",
    "        #linear_recon = np.copy(residual_recon)\n",
    "        \n",
    "        target_x_end_kspace = dim_kspaceUnd_X - target_x_start;            \n",
    "        \n",
    "        for ind_acc in range(0,acc_rate-1):\n",
    "            target_y_start = \\\n",
    "                np.int32((np.ceil(kernel_y_1/2)-1) + np.int32((np.ceil(kernel_y_2/2)-1)) + \\\n",
    "                np.int32(np.ceil(kernel_last_y/2)-1)) * acc_rate + ind_acc + 1; \n",
    "            \n",
    "            target_y_end_kspace = \\\n",
    "                dim_kspaceUnd_Y - np.int32((np.floor(kernel_y_1/2)) + (np.floor(kernel_y_2/2)) + np.floor(kernel_last_y/2)) * acc_rate + ind_acc;\n",
    "               \n",
    "            kspace_recon[0,target_x_start:target_x_end_kspace,target_y_start:target_y_end_kspace+1:acc_rate,ind_c] = \\\n",
    "                linear_recon[0,:,::acc_rate,ind_acc] + residual_recon[0,:,::acc_rate,ind_acc];\n",
    "            kspace_recon_linear[0,target_x_start:target_x_end_kspace,target_y_start:target_y_end_kspace+1:acc_rate,ind_c] \\\n",
    "                = linear_recon[0,:,::acc_rate,ind_acc];\n",
    "            kspace_recon_residual[0,target_x_start:target_x_end_kspace,target_y_start:target_y_end_kspace+1:acc_rate,ind_c] = \\\n",
    "                residual_recon[0,:,::acc_rate,ind_acc]\n",
    "            \n",
    "    kspace_recon          = np.squeeze(kspace_recon)\n",
    "    kspace_recon_linear   = np.squeeze(kspace_recon_linear)\n",
    "    kspace_recon_residual = np.squeeze(kspace_recon_residual)\n",
    "    \n",
    "    kspace_recon_complex = (kspace_recon[:,:,0:np.int32(no_channels/2)] + \\\n",
    "                np.multiply(kspace_recon[:,:,np.int32(no_channels/2):no_channels],1j))\n",
    "\n",
    "    kspace_recon_complex_linear = (kspace_recon_linear[:,:,0:np.int32(no_channels/2)] + \\\n",
    "                np.multiply(kspace_recon_linear[:,:,np.int32(no_channels/2):no_channels],1j))\n",
    "\n",
    "    kspace_recon_complex_residual = (kspace_recon_residual[:,:,0:np.int32(no_channels/2)] + \\\n",
    "                np.multiply(kspace_recon_residual[:,:,np.int32(no_channels/2):no_channels],1j))\n",
    "    \n",
    "    kspace_recon_all_nocenter[:,:,:] = np.copy(kspace_recon_complex); \n",
    "    \n",
    "    kspace_recon_complex_acs = np.copy(kspace_recon_complex);\n",
    "    kspace_recon_complex_acs[:,center_start:center_end,:] = kspace_NEVER_TOUCH[:,center_start:center_end,:]\n",
    "\n",
    "    kspace_recon_complex_linear_acs = np.copy(kspace_recon_complex_linear)\n",
    "    kspace_recon_complex_linear_acs[:,center_start:center_end,:] = \\\n",
    "        kspace_NEVER_TOUCH[:,center_start:center_end,:]\n",
    "\n",
    "        \n",
    "    return [kspace_recon_complex,kspace_recon_complex_acs,\\\n",
    "            kspace_recon_complex_linear,kspace_recon_complex_linear_acs,\\\n",
    "            kspace_recon_complex_residual,cur_kspace_truth_raki]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### perforing rraki reconstructions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Training 1/10 || Ry 5 || Acsy 20\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "ACS signal found in the input data\n",
      "go!\n",
      "learning channel # 1\n",
      "The 0 th iteration gives an error 0.09506537\n",
      "The 100 th iteration gives an error 0.0044419398\n",
      "The 200 th iteration gives an error 0.003182366\n",
      "The 300 th iteration gives an error 0.0027491986\n",
      "The 400 th iteration gives an error 0.0024614115\n",
      "The 500 th iteration gives an error 0.002278151\n",
      "The 600 th iteration gives an error 0.0021487987\n",
      "The 700 th iteration gives an error 0.0020432384\n",
      "The 800 th iteration gives an error 0.002031209\n",
      "The 900 th iteration gives an error 0.0019480456\n",
      "Time Cost: 41.30243277549744 s\n",
      "Norm of Error =  0.0018904072\n",
      "learning channel # 2\n",
      "The 0 th iteration gives an error 0.10017571\n",
      "The 100 th iteration gives an error 0.0043185577\n",
      "The 200 th iteration gives an error 0.0031654616\n",
      "The 300 th iteration gives an error 0.0027962942\n",
      "The 400 th iteration gives an error 0.0025299033\n",
      "The 500 th iteration gives an error 0.0023175674\n",
      "The 600 th iteration gives an error 0.002166272\n",
      "The 700 th iteration gives an error 0.002080037\n",
      "The 800 th iteration gives an error 0.0020057852\n",
      "The 900 th iteration gives an error 0.0019648916\n",
      "Time Cost: 3.4420676231384277 s\n",
      "Norm of Error =  0.0018992433\n",
      "learning channel # 3\n",
      "The 0 th iteration gives an error 0.0800184\n",
      "The 100 th iteration gives an error 0.004112196\n",
      "The 200 th iteration gives an error 0.0031274825\n",
      "The 300 th iteration gives an error 0.0026906068\n",
      "The 400 th iteration gives an error 0.0025020007\n",
      "The 500 th iteration gives an error 0.0024145702\n",
      "The 600 th iteration gives an error 0.002280722\n",
      "The 700 th iteration gives an error 0.002145532\n",
      "The 800 th iteration gives an error 0.0020227798\n",
      "The 900 th iteration gives an error 0.0020343005\n",
      "Time Cost: 3.458324909210205 s\n",
      "Norm of Error =  0.0019452968\n",
      "learning channel # 4\n",
      "The 0 th iteration gives an error 0.07502524\n",
      "The 100 th iteration gives an error 0.0033646324\n",
      "The 200 th iteration gives an error 0.002557619\n",
      "The 300 th iteration gives an error 0.0022022314\n",
      "The 400 th iteration gives an error 0.0019516989\n",
      "The 500 th iteration gives an error 0.0018624293\n",
      "The 600 th iteration gives an error 0.001815468\n",
      "The 700 th iteration gives an error 0.001701186\n",
      "The 800 th iteration gives an error 0.0016258863\n",
      "The 900 th iteration gives an error 0.0015513147\n",
      "Time Cost: 3.447425365447998 s\n",
      "Norm of Error =  0.0014990796\n",
      "learning channel # 5\n",
      "The 0 th iteration gives an error 0.08878064\n",
      "The 100 th iteration gives an error 0.0045276224\n",
      "The 200 th iteration gives an error 0.0032466406\n",
      "The 300 th iteration gives an error 0.0027987212\n",
      "The 400 th iteration gives an error 0.002566405\n",
      "The 500 th iteration gives an error 0.002401146\n",
      "The 600 th iteration gives an error 0.0022890852\n",
      "The 700 th iteration gives an error 0.0021747686\n",
      "The 800 th iteration gives an error 0.0020976765\n",
      "The 900 th iteration gives an error 0.0020067499\n",
      "Time Cost: 3.5918703079223633 s\n",
      "Norm of Error =  0.001981449\n",
      "learning channel # 6\n",
      "The 0 th iteration gives an error 0.09079087\n",
      "The 100 th iteration gives an error 0.004010288\n",
      "The 200 th iteration gives an error 0.0030268042\n",
      "The 300 th iteration gives an error 0.0026260666\n",
      "The 400 th iteration gives an error 0.0023943689\n",
      "The 500 th iteration gives an error 0.0022384194\n",
      "The 600 th iteration gives an error 0.0020663643\n",
      "The 700 th iteration gives an error 0.0020215986\n",
      "The 800 th iteration gives an error 0.001919429\n",
      "The 900 th iteration gives an error 0.0018814732\n",
      "Time Cost: 3.521587610244751 s\n",
      "Norm of Error =  0.0018202263\n",
      "learning channel # 7\n",
      "The 0 th iteration gives an error 0.1053261\n",
      "The 100 th iteration gives an error 0.0036346132\n",
      "The 200 th iteration gives an error 0.0027930178\n",
      "The 300 th iteration gives an error 0.0024972346\n",
      "The 400 th iteration gives an error 0.0022431836\n",
      "The 500 th iteration gives an error 0.0021236553\n",
      "The 600 th iteration gives an error 0.0020197232\n",
      "The 700 th iteration gives an error 0.00193327\n",
      "The 800 th iteration gives an error 0.0019084393\n",
      "The 900 th iteration gives an error 0.0018643993\n",
      "Time Cost: 3.5920307636260986 s\n",
      "Norm of Error =  0.0018236742\n",
      "learning channel # 8\n",
      "The 0 th iteration gives an error 0.087566994\n",
      "The 100 th iteration gives an error 0.00398186\n",
      "The 200 th iteration gives an error 0.003090872\n",
      "The 300 th iteration gives an error 0.0027368017\n",
      "The 400 th iteration gives an error 0.0025259685\n",
      "The 500 th iteration gives an error 0.0023943456\n",
      "The 600 th iteration gives an error 0.0022941935\n",
      "The 700 th iteration gives an error 0.002253922\n",
      "The 800 th iteration gives an error 0.0021888884\n",
      "The 900 th iteration gives an error 0.0021357848\n",
      "Time Cost: 3.4703962802886963 s\n",
      "Norm of Error =  0.0020750128\n",
      "learning channel # 9\n",
      "The 0 th iteration gives an error 0.08483033\n",
      "The 100 th iteration gives an error 0.003532388\n",
      "The 200 th iteration gives an error 0.0026435931\n",
      "The 300 th iteration gives an error 0.0022637355\n",
      "The 400 th iteration gives an error 0.0019712632\n",
      "The 500 th iteration gives an error 0.0019003231\n",
      "The 600 th iteration gives an error 0.001758\n",
      "The 700 th iteration gives an error 0.0016739416\n",
      "The 800 th iteration gives an error 0.0015861965\n",
      "The 900 th iteration gives an error 0.0015403124\n",
      "Time Cost: 3.496752977371216 s\n",
      "Norm of Error =  0.0014714028\n",
      "learning channel # 10\n",
      "The 0 th iteration gives an error 0.093857154\n",
      "The 100 th iteration gives an error 0.00377738\n",
      "The 200 th iteration gives an error 0.0028195735\n",
      "The 300 th iteration gives an error 0.0024097874\n",
      "The 400 th iteration gives an error 0.0020986723\n",
      "The 500 th iteration gives an error 0.0019860317\n",
      "The 600 th iteration gives an error 0.0019394669\n",
      "The 700 th iteration gives an error 0.0018133454\n",
      "The 800 th iteration gives an error 0.0017049643\n",
      "The 900 th iteration gives an error 0.0016887619\n",
      "Time Cost: 3.625951051712036 s\n",
      "Norm of Error =  0.0016476021\n",
      "learning channel # 11\n",
      "The 0 th iteration gives an error 0.0951044\n",
      "The 100 th iteration gives an error 0.004342168\n",
      "The 200 th iteration gives an error 0.0031945405\n",
      "The 300 th iteration gives an error 0.00266157\n",
      "The 400 th iteration gives an error 0.0023960485\n",
      "The 500 th iteration gives an error 0.0023002033\n",
      "The 600 th iteration gives an error 0.0021529354\n",
      "The 700 th iteration gives an error 0.002072051\n",
      "The 800 th iteration gives an error 0.0019918005\n",
      "The 900 th iteration gives an error 0.0019264945\n",
      "Time Cost: 3.5734689235687256 s\n",
      "Norm of Error =  0.001871103\n",
      "learning channel # 12\n",
      "The 0 th iteration gives an error 0.08472447\n",
      "The 100 th iteration gives an error 0.0038080271\n",
      "The 200 th iteration gives an error 0.0028112796\n",
      "The 300 th iteration gives an error 0.002429415\n",
      "The 400 th iteration gives an error 0.0021139462\n",
      "The 500 th iteration gives an error 0.0019857732\n",
      "The 600 th iteration gives an error 0.0018339627\n",
      "The 700 th iteration gives an error 0.0018384017\n",
      "The 800 th iteration gives an error 0.0017177245\n",
      "The 900 th iteration gives an error 0.0016596414\n",
      "Time Cost: 3.591770648956299 s\n",
      "Norm of Error =  0.0016042013\n",
      "learning channel # 13\n",
      "The 0 th iteration gives an error 0.075796664\n",
      "The 100 th iteration gives an error 0.0036019322\n",
      "The 200 th iteration gives an error 0.0027919845\n",
      "The 300 th iteration gives an error 0.0024420372\n",
      "The 400 th iteration gives an error 0.0022643218\n",
      "The 500 th iteration gives an error 0.0021220301\n",
      "The 600 th iteration gives an error 0.00196979\n",
      "The 700 th iteration gives an error 0.0019107993\n",
      "The 800 th iteration gives an error 0.0018196513\n",
      "The 900 th iteration gives an error 0.0017742381\n",
      "Time Cost: 3.551313638687134 s\n",
      "Norm of Error =  0.0017027424\n",
      "learning channel # 14\n",
      "The 0 th iteration gives an error 0.0824988\n",
      "The 100 th iteration gives an error 0.003357833\n",
      "The 200 th iteration gives an error 0.0024833572\n",
      "The 300 th iteration gives an error 0.0021366733\n",
      "The 400 th iteration gives an error 0.0019209972\n",
      "The 500 th iteration gives an error 0.0017377472\n",
      "The 600 th iteration gives an error 0.0016524267\n",
      "The 700 th iteration gives an error 0.0015800339\n",
      "The 800 th iteration gives an error 0.0015366118\n",
      "The 900 th iteration gives an error 0.0014820895\n",
      "Time Cost: 3.4653656482696533 s\n",
      "Norm of Error =  0.0013951797\n",
      "learning channel # 15\n",
      "The 0 th iteration gives an error 0.09302532\n",
      "The 100 th iteration gives an error 0.004453225\n",
      "The 200 th iteration gives an error 0.0031781024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 300 th iteration gives an error 0.0027335868\n",
      "The 400 th iteration gives an error 0.0023968956\n",
      "The 500 th iteration gives an error 0.002249312\n",
      "The 600 th iteration gives an error 0.0021704487\n",
      "The 700 th iteration gives an error 0.0020970602\n",
      "The 800 th iteration gives an error 0.0019862982\n",
      "The 900 th iteration gives an error 0.001969071\n",
      "Time Cost: 3.749126672744751 s\n",
      "Norm of Error =  0.0019006375\n",
      "learning channel # 16\n",
      "The 0 th iteration gives an error 0.09148891\n",
      "The 100 th iteration gives an error 0.0042047543\n",
      "The 200 th iteration gives an error 0.0029646151\n",
      "The 300 th iteration gives an error 0.0025388168\n",
      "The 400 th iteration gives an error 0.0022457084\n",
      "The 500 th iteration gives an error 0.0021381073\n",
      "The 600 th iteration gives an error 0.0019668434\n",
      "The 700 th iteration gives an error 0.001909819\n",
      "The 800 th iteration gives an error 0.0018042708\n",
      "The 900 th iteration gives an error 0.0017215719\n",
      "Time Cost: 3.586442708969116 s\n",
      "Norm of Error =  0.0017382996\n",
      "learning channel # 17\n",
      "The 0 th iteration gives an error 0.0868424\n",
      "The 100 th iteration gives an error 0.0041486006\n",
      "The 200 th iteration gives an error 0.0030488865\n",
      "The 300 th iteration gives an error 0.0026554645\n",
      "The 400 th iteration gives an error 0.00245082\n",
      "The 500 th iteration gives an error 0.0022126029\n",
      "The 600 th iteration gives an error 0.0021166655\n",
      "The 700 th iteration gives an error 0.0020358972\n",
      "The 800 th iteration gives an error 0.0019470057\n",
      "The 900 th iteration gives an error 0.0018856296\n",
      "Time Cost: 3.5228018760681152 s\n",
      "Norm of Error =  0.0018717408\n",
      "learning channel # 18\n",
      "The 0 th iteration gives an error 0.08094451\n",
      "The 100 th iteration gives an error 0.0037160162\n",
      "The 200 th iteration gives an error 0.002681301\n",
      "The 300 th iteration gives an error 0.002262055\n",
      "The 400 th iteration gives an error 0.001936688\n",
      "The 500 th iteration gives an error 0.0019000941\n",
      "The 600 th iteration gives an error 0.0016622093\n",
      "The 700 th iteration gives an error 0.001626869\n",
      "The 800 th iteration gives an error 0.0015577581\n",
      "The 900 th iteration gives an error 0.0015413966\n",
      "Time Cost: 3.5008647441864014 s\n",
      "Norm of Error =  0.0014067653\n",
      "learning channel # 19\n",
      "The 0 th iteration gives an error 0.10628623\n",
      "The 100 th iteration gives an error 0.004691313\n",
      "The 200 th iteration gives an error 0.0033264724\n",
      "The 300 th iteration gives an error 0.0028553233\n",
      "The 400 th iteration gives an error 0.0026106532\n",
      "The 500 th iteration gives an error 0.002419474\n",
      "The 600 th iteration gives an error 0.0022425181\n",
      "The 700 th iteration gives an error 0.0021376582\n",
      "The 800 th iteration gives an error 0.0020580264\n",
      "The 900 th iteration gives an error 0.0019832691\n",
      "Time Cost: 3.5639185905456543 s\n",
      "Norm of Error =  0.001925823\n",
      "learning channel # 20\n",
      "The 0 th iteration gives an error 0.08541072\n",
      "The 100 th iteration gives an error 0.0038820268\n",
      "The 200 th iteration gives an error 0.0028147711\n",
      "The 300 th iteration gives an error 0.0024369792\n",
      "The 400 th iteration gives an error 0.0022273543\n",
      "The 500 th iteration gives an error 0.002061686\n",
      "The 600 th iteration gives an error 0.0018709851\n",
      "The 700 th iteration gives an error 0.001805339\n",
      "The 800 th iteration gives an error 0.0017461944\n",
      "The 900 th iteration gives an error 0.0016629081\n",
      "Time Cost: 3.5926172733306885 s\n",
      "Norm of Error =  0.0016049815\n",
      "learning channel # 21\n",
      "The 0 th iteration gives an error 0.10478343\n",
      "The 100 th iteration gives an error 0.0053453445\n",
      "The 200 th iteration gives an error 0.0035124542\n",
      "The 300 th iteration gives an error 0.0029643192\n",
      "The 400 th iteration gives an error 0.0026721116\n",
      "The 500 th iteration gives an error 0.0024832578\n",
      "The 600 th iteration gives an error 0.0023032362\n",
      "The 700 th iteration gives an error 0.0022017572\n",
      "The 800 th iteration gives an error 0.002116654\n",
      "The 900 th iteration gives an error 0.002058613\n",
      "Time Cost: 3.562769889831543 s\n",
      "Norm of Error =  0.002007077\n",
      "learning channel # 22\n",
      "The 0 th iteration gives an error 0.09489226\n",
      "The 100 th iteration gives an error 0.0040064445\n",
      "The 200 th iteration gives an error 0.0028268672\n",
      "The 300 th iteration gives an error 0.0023660362\n",
      "The 400 th iteration gives an error 0.0021830648\n",
      "The 500 th iteration gives an error 0.001969703\n",
      "The 600 th iteration gives an error 0.0018897423\n",
      "The 700 th iteration gives an error 0.0018391232\n",
      "The 800 th iteration gives an error 0.0017486839\n",
      "The 900 th iteration gives an error 0.001686992\n",
      "Time Cost: 3.5595202445983887 s\n",
      "Norm of Error =  0.0016819048\n",
      "learning channel # 23\n",
      "The 0 th iteration gives an error 0.095069475\n",
      "The 100 th iteration gives an error 0.0051230895\n",
      "The 200 th iteration gives an error 0.0036844877\n",
      "The 300 th iteration gives an error 0.0032118903\n",
      "The 400 th iteration gives an error 0.0028388307\n",
      "The 500 th iteration gives an error 0.002690422\n",
      "The 600 th iteration gives an error 0.0025427544\n",
      "The 700 th iteration gives an error 0.0024199015\n",
      "The 800 th iteration gives an error 0.0023110246\n",
      "The 900 th iteration gives an error 0.0022338412\n",
      "Time Cost: 3.4952213764190674 s\n",
      "Norm of Error =  0.0022179342\n",
      "learning channel # 24\n",
      "The 0 th iteration gives an error 0.077594206\n",
      "The 100 th iteration gives an error 0.003446009\n",
      "The 200 th iteration gives an error 0.0025740333\n",
      "The 300 th iteration gives an error 0.0021747923\n",
      "The 400 th iteration gives an error 0.0018944766\n",
      "The 500 th iteration gives an error 0.0017992089\n",
      "The 600 th iteration gives an error 0.0016725552\n",
      "The 700 th iteration gives an error 0.0016199284\n",
      "The 800 th iteration gives an error 0.0015274147\n",
      "The 900 th iteration gives an error 0.0015110231\n",
      "Time Cost: 3.485177755355835 s\n",
      "Norm of Error =  0.0014107156\n",
      "learning channel # 25\n",
      "The 0 th iteration gives an error 0.13033307\n",
      "The 100 th iteration gives an error 0.0065196054\n",
      "The 200 th iteration gives an error 0.0038135536\n",
      "The 300 th iteration gives an error 0.0031812983\n",
      "The 400 th iteration gives an error 0.002822222\n",
      "The 500 th iteration gives an error 0.0026535096\n",
      "The 600 th iteration gives an error 0.0024901237\n",
      "The 700 th iteration gives an error 0.0024047662\n",
      "The 800 th iteration gives an error 0.0023356033\n",
      "The 900 th iteration gives an error 0.002195891\n",
      "Time Cost: 3.581869602203369 s\n",
      "Norm of Error =  0.00219505\n",
      "learning channel # 26\n",
      "The 0 th iteration gives an error 0.08684488\n",
      "The 100 th iteration gives an error 0.0040641343\n",
      "The 200 th iteration gives an error 0.0029886183\n",
      "The 300 th iteration gives an error 0.002444994\n",
      "The 400 th iteration gives an error 0.0022794912\n",
      "The 500 th iteration gives an error 0.0020763802\n",
      "The 600 th iteration gives an error 0.001977779\n",
      "The 700 th iteration gives an error 0.0018668312\n",
      "The 800 th iteration gives an error 0.0017644906\n",
      "The 900 th iteration gives an error 0.0017492734\n",
      "Time Cost: 3.5358567237854004 s\n",
      "Norm of Error =  0.0016817558\n",
      "learning channel # 27\n",
      "The 0 th iteration gives an error 0.14645278\n",
      "The 100 th iteration gives an error 0.008107383\n",
      "The 200 th iteration gives an error 0.0045963563\n",
      "The 300 th iteration gives an error 0.0037815846\n",
      "The 400 th iteration gives an error 0.0034271004\n",
      "The 500 th iteration gives an error 0.003200707\n",
      "The 600 th iteration gives an error 0.0029377858\n",
      "The 700 th iteration gives an error 0.0028111045\n",
      "The 800 th iteration gives an error 0.0027059787\n",
      "The 900 th iteration gives an error 0.0026464025\n",
      "Time Cost: 3.5555310249328613 s\n",
      "Norm of Error =  0.0025657588\n",
      "learning channel # 28\n",
      "The 0 th iteration gives an error 0.089403436\n",
      "The 100 th iteration gives an error 0.0040687723\n",
      "The 200 th iteration gives an error 0.0029266158\n",
      "The 300 th iteration gives an error 0.0024669361\n",
      "The 400 th iteration gives an error 0.0022438192\n",
      "The 500 th iteration gives an error 0.0020305524\n",
      "The 600 th iteration gives an error 0.0018831379\n",
      "The 700 th iteration gives an error 0.0018519491\n",
      "The 800 th iteration gives an error 0.001758104\n",
      "The 900 th iteration gives an error 0.0017826523\n",
      "Time Cost: 3.511272430419922 s\n",
      "Norm of Error =  0.0017158707\n",
      "learning channel # 29\n",
      "The 0 th iteration gives an error 0.084013894\n",
      "The 100 th iteration gives an error 0.0036633995\n",
      "The 200 th iteration gives an error 0.0028630248\n",
      "The 300 th iteration gives an error 0.0024949608\n",
      "The 400 th iteration gives an error 0.0023072693\n",
      "The 500 th iteration gives an error 0.002098717\n",
      "The 600 th iteration gives an error 0.001985146\n",
      "The 700 th iteration gives an error 0.0018940859\n",
      "The 800 th iteration gives an error 0.0018342842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 900 th iteration gives an error 0.0018251757\n",
      "Time Cost: 3.543794631958008 s\n",
      "Norm of Error =  0.0017361259\n",
      "learning channel # 30\n",
      "The 0 th iteration gives an error 0.09187326\n",
      "The 100 th iteration gives an error 0.003571893\n",
      "The 200 th iteration gives an error 0.0026381682\n",
      "The 300 th iteration gives an error 0.0022333637\n",
      "The 400 th iteration gives an error 0.0020251446\n",
      "The 500 th iteration gives an error 0.0018520684\n",
      "The 600 th iteration gives an error 0.0017769295\n",
      "The 700 th iteration gives an error 0.0016298769\n",
      "The 800 th iteration gives an error 0.0015820565\n",
      "The 900 th iteration gives an error 0.0015028722\n",
      "Time Cost: 3.533731698989868 s\n",
      "Norm of Error =  0.0014648675\n",
      "learning channel # 31\n",
      "The 0 th iteration gives an error 0.10765435\n",
      "The 100 th iteration gives an error 0.0050955005\n",
      "The 200 th iteration gives an error 0.0037243045\n",
      "The 300 th iteration gives an error 0.0032175342\n",
      "The 400 th iteration gives an error 0.0029493095\n",
      "The 500 th iteration gives an error 0.0027533344\n",
      "The 600 th iteration gives an error 0.0026148558\n",
      "The 700 th iteration gives an error 0.0025109896\n",
      "The 800 th iteration gives an error 0.0023800689\n",
      "The 900 th iteration gives an error 0.0023574182\n",
      "Time Cost: 3.5148165225982666 s\n",
      "Norm of Error =  0.002290845\n",
      "learning channel # 32\n",
      "The 0 th iteration gives an error 0.11540954\n",
      "The 100 th iteration gives an error 0.005761019\n",
      "The 200 th iteration gives an error 0.0037232747\n",
      "The 300 th iteration gives an error 0.0031029154\n",
      "The 400 th iteration gives an error 0.0027889821\n",
      "The 500 th iteration gives an error 0.002675588\n",
      "The 600 th iteration gives an error 0.0024642958\n",
      "The 700 th iteration gives an error 0.0023168977\n",
      "The 800 th iteration gives an error 0.002224722\n",
      "The 900 th iteration gives an error 0.0021473137\n",
      "Time Cost: 3.3978841304779053 s\n",
      "Norm of Error =  0.0021010342\n",
      "learning channel # 33\n",
      "The 0 th iteration gives an error 0.09213664\n",
      "The 100 th iteration gives an error 0.0044097733\n",
      "The 200 th iteration gives an error 0.003191838\n",
      "The 300 th iteration gives an error 0.0027854973\n",
      "The 400 th iteration gives an error 0.0024419108\n",
      "The 500 th iteration gives an error 0.0022801026\n",
      "The 600 th iteration gives an error 0.0021203808\n",
      "The 700 th iteration gives an error 0.0019747394\n",
      "The 800 th iteration gives an error 0.001941304\n",
      "The 900 th iteration gives an error 0.0018515983\n",
      "Time Cost: 3.473996639251709 s\n",
      "Norm of Error =  0.001811862\n",
      "learning channel # 34\n",
      "The 0 th iteration gives an error 0.08519094\n",
      "The 100 th iteration gives an error 0.0043230737\n",
      "The 200 th iteration gives an error 0.0031091115\n",
      "The 300 th iteration gives an error 0.0027007223\n",
      "The 400 th iteration gives an error 0.002416267\n",
      "The 500 th iteration gives an error 0.0022659176\n",
      "The 600 th iteration gives an error 0.0021745036\n",
      "The 700 th iteration gives an error 0.0020428798\n",
      "The 800 th iteration gives an error 0.0019908464\n",
      "The 900 th iteration gives an error 0.0019562494\n",
      "Time Cost: 3.4707353115081787 s\n",
      "Norm of Error =  0.0019012495\n",
      "learning channel # 35\n",
      "The 0 th iteration gives an error 0.09622075\n",
      "The 100 th iteration gives an error 0.004013934\n",
      "The 200 th iteration gives an error 0.0030577402\n",
      "The 300 th iteration gives an error 0.0026907874\n",
      "The 400 th iteration gives an error 0.0024924567\n",
      "The 500 th iteration gives an error 0.0023397598\n",
      "The 600 th iteration gives an error 0.002178053\n",
      "The 700 th iteration gives an error 0.0021439232\n",
      "The 800 th iteration gives an error 0.0020704307\n",
      "The 900 th iteration gives an error 0.0020084279\n",
      "Time Cost: 3.453049898147583 s\n",
      "Norm of Error =  0.0019720166\n",
      "learning channel # 36\n",
      "The 0 th iteration gives an error 0.08677237\n",
      "The 100 th iteration gives an error 0.003574314\n",
      "The 200 th iteration gives an error 0.0025793551\n",
      "The 300 th iteration gives an error 0.0022573185\n",
      "The 400 th iteration gives an error 0.0019948685\n",
      "The 500 th iteration gives an error 0.0018006328\n",
      "The 600 th iteration gives an error 0.0017584756\n",
      "The 700 th iteration gives an error 0.001652929\n",
      "The 800 th iteration gives an error 0.0015927596\n",
      "The 900 th iteration gives an error 0.0015461177\n",
      "Time Cost: 3.6476449966430664 s\n",
      "Norm of Error =  0.0014808705\n",
      "learning channel # 37\n",
      "The 0 th iteration gives an error 0.095533475\n",
      "The 100 th iteration gives an error 0.0044000563\n",
      "The 200 th iteration gives an error 0.0032593952\n",
      "The 300 th iteration gives an error 0.002793244\n",
      "The 400 th iteration gives an error 0.002526958\n",
      "The 500 th iteration gives an error 0.0024119266\n",
      "The 600 th iteration gives an error 0.0022892077\n",
      "The 700 th iteration gives an error 0.0021638046\n",
      "The 800 th iteration gives an error 0.0021108733\n",
      "The 900 th iteration gives an error 0.0019959055\n",
      "Time Cost: 3.516936779022217 s\n",
      "Norm of Error =  0.0019659689\n",
      "learning channel # 38\n",
      "The 0 th iteration gives an error 0.090121\n",
      "The 100 th iteration gives an error 0.00396908\n",
      "The 200 th iteration gives an error 0.0029609925\n",
      "The 300 th iteration gives an error 0.002560276\n",
      "The 400 th iteration gives an error 0.002321586\n",
      "The 500 th iteration gives an error 0.0021967795\n",
      "The 600 th iteration gives an error 0.0020625654\n",
      "The 700 th iteration gives an error 0.0019893262\n",
      "The 800 th iteration gives an error 0.0019495429\n",
      "The 900 th iteration gives an error 0.0018860141\n",
      "Time Cost: 3.4961154460906982 s\n",
      "Norm of Error =  0.0018139051\n",
      "learning channel # 39\n",
      "The 0 th iteration gives an error 0.07430227\n",
      "The 100 th iteration gives an error 0.0037708012\n",
      "The 200 th iteration gives an error 0.0028870516\n",
      "The 300 th iteration gives an error 0.0025088324\n",
      "The 400 th iteration gives an error 0.0023245844\n",
      "The 500 th iteration gives an error 0.0021979073\n",
      "The 600 th iteration gives an error 0.0020530587\n",
      "The 700 th iteration gives an error 0.0019557711\n",
      "The 800 th iteration gives an error 0.0019000621\n",
      "The 900 th iteration gives an error 0.0018563432\n",
      "Time Cost: 3.5519027709960938 s\n",
      "Norm of Error =  0.0018099878\n",
      "learning channel # 40\n",
      "The 0 th iteration gives an error 0.08531048\n",
      "The 100 th iteration gives an error 0.00368803\n",
      "The 200 th iteration gives an error 0.0029371418\n",
      "The 300 th iteration gives an error 0.0026721177\n",
      "The 400 th iteration gives an error 0.0024888264\n",
      "The 500 th iteration gives an error 0.002312413\n",
      "The 600 th iteration gives an error 0.002243802\n",
      "The 700 th iteration gives an error 0.002206881\n",
      "The 800 th iteration gives an error 0.0021160194\n",
      "The 900 th iteration gives an error 0.0020374996\n",
      "Time Cost: 3.5466554164886475 s\n",
      "Norm of Error =  0.0019995903\n",
      "learning channel # 41\n",
      "The 0 th iteration gives an error 0.08629915\n",
      "The 100 th iteration gives an error 0.0035055652\n",
      "The 200 th iteration gives an error 0.002656533\n",
      "The 300 th iteration gives an error 0.0023178114\n",
      "The 400 th iteration gives an error 0.0020914748\n",
      "The 500 th iteration gives an error 0.0020548555\n",
      "The 600 th iteration gives an error 0.001838948\n",
      "The 700 th iteration gives an error 0.0017338824\n",
      "The 800 th iteration gives an error 0.0016964881\n",
      "The 900 th iteration gives an error 0.0016298292\n",
      "Time Cost: 3.503157138824463 s\n",
      "Norm of Error =  0.0015605049\n",
      "learning channel # 42\n",
      "The 0 th iteration gives an error 0.090800986\n",
      "The 100 th iteration gives an error 0.0037819243\n",
      "The 200 th iteration gives an error 0.002748331\n",
      "The 300 th iteration gives an error 0.0023124865\n",
      "The 400 th iteration gives an error 0.0020973329\n",
      "The 500 th iteration gives an error 0.0019570852\n",
      "The 600 th iteration gives an error 0.0018591094\n",
      "The 700 th iteration gives an error 0.0017592505\n",
      "The 800 th iteration gives an error 0.0016594752\n",
      "The 900 th iteration gives an error 0.0016402508\n",
      "Time Cost: 3.561424493789673 s\n",
      "Norm of Error =  0.0016043312\n",
      "learning channel # 43\n",
      "The 0 th iteration gives an error 0.09644268\n",
      "The 100 th iteration gives an error 0.004760839\n",
      "The 200 th iteration gives an error 0.0031537083\n",
      "The 300 th iteration gives an error 0.0027166097\n",
      "The 400 th iteration gives an error 0.0024953466\n",
      "The 500 th iteration gives an error 0.0022598926\n",
      "The 600 th iteration gives an error 0.0021447989\n",
      "The 700 th iteration gives an error 0.0020642187\n",
      "The 800 th iteration gives an error 0.0019901264\n",
      "The 900 th iteration gives an error 0.0019298603\n",
      "Time Cost: 3.456392288208008 s\n",
      "Norm of Error =  0.0018936874\n",
      "learning channel # 44\n",
      "The 0 th iteration gives an error 0.09224216\n",
      "The 100 th iteration gives an error 0.004103943\n",
      "The 200 th iteration gives an error 0.002958371\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 300 th iteration gives an error 0.0025440224\n",
      "The 400 th iteration gives an error 0.0022768746\n",
      "The 500 th iteration gives an error 0.0021622719\n",
      "The 600 th iteration gives an error 0.0020269514\n",
      "The 700 th iteration gives an error 0.0018883445\n",
      "The 800 th iteration gives an error 0.0018332351\n",
      "The 900 th iteration gives an error 0.0017343468\n",
      "Time Cost: 3.5506982803344727 s\n",
      "Norm of Error =  0.001707549\n",
      "learning channel # 45\n",
      "The 0 th iteration gives an error 0.08406578\n",
      "The 100 th iteration gives an error 0.003683675\n",
      "The 200 th iteration gives an error 0.002882029\n",
      "The 300 th iteration gives an error 0.0024672365\n",
      "The 400 th iteration gives an error 0.002264986\n",
      "The 500 th iteration gives an error 0.0021205484\n",
      "The 600 th iteration gives an error 0.0019787848\n",
      "The 700 th iteration gives an error 0.0019693403\n",
      "The 800 th iteration gives an error 0.0018673411\n",
      "The 900 th iteration gives an error 0.0017544356\n",
      "Time Cost: 3.4994637966156006 s\n",
      "Norm of Error =  0.001837212\n",
      "learning channel # 46\n",
      "The 0 th iteration gives an error 0.0966108\n",
      "The 100 th iteration gives an error 0.0036406117\n",
      "The 200 th iteration gives an error 0.0026268107\n",
      "The 300 th iteration gives an error 0.0021457837\n",
      "The 400 th iteration gives an error 0.0018705048\n",
      "The 500 th iteration gives an error 0.0017665271\n",
      "The 600 th iteration gives an error 0.0016459059\n",
      "The 700 th iteration gives an error 0.0016142309\n",
      "The 800 th iteration gives an error 0.0015667182\n",
      "The 900 th iteration gives an error 0.0015304885\n",
      "Time Cost: 3.5175952911376953 s\n",
      "Norm of Error =  0.0013940865\n",
      "learning channel # 47\n",
      "The 0 th iteration gives an error 0.0975789\n",
      "The 100 th iteration gives an error 0.0043530622\n",
      "The 200 th iteration gives an error 0.0030900324\n",
      "The 300 th iteration gives an error 0.002666322\n",
      "The 400 th iteration gives an error 0.0023757666\n",
      "The 500 th iteration gives an error 0.0022825012\n",
      "The 600 th iteration gives an error 0.0021302039\n",
      "The 700 th iteration gives an error 0.0020046022\n",
      "The 800 th iteration gives an error 0.001947399\n",
      "The 900 th iteration gives an error 0.0018849364\n",
      "Time Cost: 3.5109710693359375 s\n",
      "Norm of Error =  0.0018392229\n",
      "learning channel # 48\n",
      "The 0 th iteration gives an error 0.09195476\n",
      "The 100 th iteration gives an error 0.0042754756\n",
      "The 200 th iteration gives an error 0.0029738285\n",
      "The 300 th iteration gives an error 0.0025571906\n",
      "The 400 th iteration gives an error 0.0022941437\n",
      "The 500 th iteration gives an error 0.0021219\n",
      "The 600 th iteration gives an error 0.001954455\n",
      "The 700 th iteration gives an error 0.001908528\n",
      "The 800 th iteration gives an error 0.0018271238\n",
      "The 900 th iteration gives an error 0.0017753956\n",
      "Time Cost: 3.5080223083496094 s\n",
      "Norm of Error =  0.0017484108\n",
      "learning channel # 49\n",
      "The 0 th iteration gives an error 0.104508474\n",
      "The 100 th iteration gives an error 0.004297695\n",
      "The 200 th iteration gives an error 0.0031147378\n",
      "The 300 th iteration gives an error 0.0026323732\n",
      "The 400 th iteration gives an error 0.0023898901\n",
      "The 500 th iteration gives an error 0.002294803\n",
      "The 600 th iteration gives an error 0.0021437781\n",
      "The 700 th iteration gives an error 0.0021163644\n",
      "The 800 th iteration gives an error 0.0019895637\n",
      "The 900 th iteration gives an error 0.0019531385\n",
      "Time Cost: 3.5576775074005127 s\n",
      "Norm of Error =  0.0019240786\n",
      "learning channel # 50\n",
      "The 0 th iteration gives an error 0.09177412\n",
      "The 100 th iteration gives an error 0.0036033436\n",
      "The 200 th iteration gives an error 0.002633335\n",
      "The 300 th iteration gives an error 0.0022474963\n",
      "The 400 th iteration gives an error 0.001974563\n",
      "The 500 th iteration gives an error 0.0018947741\n",
      "The 600 th iteration gives an error 0.0017722684\n",
      "The 700 th iteration gives an error 0.0016643882\n",
      "The 800 th iteration gives an error 0.0015873263\n",
      "The 900 th iteration gives an error 0.0015456083\n",
      "Time Cost: 3.5634777545928955 s\n",
      "Norm of Error =  0.0015079961\n",
      "learning channel # 51\n",
      "The 0 th iteration gives an error 0.093243554\n",
      "The 100 th iteration gives an error 0.0047444864\n",
      "The 200 th iteration gives an error 0.003303341\n",
      "The 300 th iteration gives an error 0.002801999\n",
      "The 400 th iteration gives an error 0.002516872\n",
      "The 500 th iteration gives an error 0.0023559732\n",
      "The 600 th iteration gives an error 0.0022711188\n",
      "The 700 th iteration gives an error 0.0021006563\n",
      "The 800 th iteration gives an error 0.0020568757\n",
      "The 900 th iteration gives an error 0.0019966746\n",
      "Time Cost: 3.470428228378296 s\n",
      "Norm of Error =  0.0019107237\n",
      "learning channel # 52\n",
      "The 0 th iteration gives an error 0.09462857\n",
      "The 100 th iteration gives an error 0.003934945\n",
      "The 200 th iteration gives an error 0.0028228043\n",
      "The 300 th iteration gives an error 0.0024429876\n",
      "The 400 th iteration gives an error 0.0021944875\n",
      "The 500 th iteration gives an error 0.0020316604\n",
      "The 600 th iteration gives an error 0.0019181534\n",
      "The 700 th iteration gives an error 0.0018755901\n",
      "The 800 th iteration gives an error 0.0017230415\n",
      "The 900 th iteration gives an error 0.0017131054\n",
      "Time Cost: 3.5242865085601807 s\n",
      "Norm of Error =  0.0016333506\n",
      "learning channel # 53\n",
      "The 0 th iteration gives an error 0.09845537\n",
      "The 100 th iteration gives an error 0.0050208247\n",
      "The 200 th iteration gives an error 0.0035483344\n",
      "The 300 th iteration gives an error 0.0029495829\n",
      "The 400 th iteration gives an error 0.0026480625\n",
      "The 500 th iteration gives an error 0.0025013767\n",
      "The 600 th iteration gives an error 0.0022952189\n",
      "The 700 th iteration gives an error 0.0022585746\n",
      "The 800 th iteration gives an error 0.0021280176\n",
      "The 900 th iteration gives an error 0.0020249742\n",
      "Time Cost: 3.4961068630218506 s\n",
      "Norm of Error =  0.001999104\n",
      "learning channel # 54\n",
      "The 0 th iteration gives an error 0.081318244\n",
      "The 100 th iteration gives an error 0.003884618\n",
      "The 200 th iteration gives an error 0.0028242047\n",
      "The 300 th iteration gives an error 0.0023986786\n",
      "The 400 th iteration gives an error 0.0021537687\n",
      "The 500 th iteration gives an error 0.002015025\n",
      "The 600 th iteration gives an error 0.0018635697\n",
      "The 700 th iteration gives an error 0.001798781\n",
      "The 800 th iteration gives an error 0.0017330076\n",
      "The 900 th iteration gives an error 0.0016497056\n",
      "Time Cost: 3.558161973953247 s\n",
      "Norm of Error =  0.0016064395\n",
      "learning channel # 55\n",
      "The 0 th iteration gives an error 0.10874856\n",
      "The 100 th iteration gives an error 0.0050379816\n",
      "The 200 th iteration gives an error 0.003611042\n",
      "The 300 th iteration gives an error 0.0031623635\n",
      "The 400 th iteration gives an error 0.0029342247\n",
      "The 500 th iteration gives an error 0.0026412122\n",
      "The 600 th iteration gives an error 0.0025524097\n",
      "The 700 th iteration gives an error 0.0024846452\n",
      "The 800 th iteration gives an error 0.0023233783\n",
      "The 900 th iteration gives an error 0.0023525679\n",
      "Time Cost: 3.524198055267334 s\n",
      "Norm of Error =  0.0022504004\n",
      "learning channel # 56\n",
      "The 0 th iteration gives an error 0.08899398\n",
      "The 100 th iteration gives an error 0.003414238\n",
      "The 200 th iteration gives an error 0.0025500176\n",
      "The 300 th iteration gives an error 0.0021245577\n",
      "The 400 th iteration gives an error 0.001962033\n",
      "The 500 th iteration gives an error 0.0018189033\n",
      "The 600 th iteration gives an error 0.0016648378\n",
      "The 700 th iteration gives an error 0.0016064106\n",
      "The 800 th iteration gives an error 0.0016379622\n",
      "The 900 th iteration gives an error 0.0014974511\n",
      "Time Cost: 3.5919792652130127 s\n",
      "Norm of Error =  0.0014640037\n",
      "learning channel # 57\n",
      "The 0 th iteration gives an error 0.10087738\n",
      "The 100 th iteration gives an error 0.005563344\n",
      "The 200 th iteration gives an error 0.0037388299\n",
      "The 300 th iteration gives an error 0.00315115\n",
      "The 400 th iteration gives an error 0.0028598541\n",
      "The 500 th iteration gives an error 0.0026153065\n",
      "The 600 th iteration gives an error 0.0024731471\n",
      "The 700 th iteration gives an error 0.002377511\n",
      "The 800 th iteration gives an error 0.0022648324\n",
      "The 900 th iteration gives an error 0.0021798334\n",
      "Time Cost: 3.501100540161133 s\n",
      "Norm of Error =  0.0021430776\n",
      "learning channel # 58\n",
      "The 0 th iteration gives an error 0.089180276\n",
      "The 100 th iteration gives an error 0.004144875\n",
      "The 200 th iteration gives an error 0.0029850025\n",
      "The 300 th iteration gives an error 0.0025233002\n",
      "The 400 th iteration gives an error 0.0022523883\n",
      "The 500 th iteration gives an error 0.002068382\n",
      "The 600 th iteration gives an error 0.0019703633\n",
      "The 700 th iteration gives an error 0.0018963437\n",
      "The 800 th iteration gives an error 0.0018146625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 900 th iteration gives an error 0.0017281404\n",
      "Time Cost: 3.603980302810669 s\n",
      "Norm of Error =  0.0017500864\n",
      "learning channel # 59\n",
      "The 0 th iteration gives an error 0.10536118\n",
      "The 100 th iteration gives an error 0.007503024\n",
      "The 200 th iteration gives an error 0.004752017\n",
      "The 300 th iteration gives an error 0.0039311093\n",
      "The 400 th iteration gives an error 0.0034560403\n",
      "The 500 th iteration gives an error 0.003143086\n",
      "The 600 th iteration gives an error 0.0029760585\n",
      "The 700 th iteration gives an error 0.0028857654\n",
      "The 800 th iteration gives an error 0.0027485043\n",
      "The 900 th iteration gives an error 0.0026949013\n",
      "Time Cost: 3.57247257232666 s\n",
      "Norm of Error =  0.0025838874\n",
      "learning channel # 60\n",
      "The 0 th iteration gives an error 0.0828723\n",
      "The 100 th iteration gives an error 0.003994931\n",
      "The 200 th iteration gives an error 0.002887839\n",
      "The 300 th iteration gives an error 0.002430635\n",
      "The 400 th iteration gives an error 0.0022209855\n",
      "The 500 th iteration gives an error 0.0020505688\n",
      "The 600 th iteration gives an error 0.001979811\n",
      "The 700 th iteration gives an error 0.0019076748\n",
      "The 800 th iteration gives an error 0.0017656358\n",
      "The 900 th iteration gives an error 0.0016989019\n",
      "Time Cost: 3.5332932472229004 s\n",
      "Norm of Error =  0.0016419106\n",
      "learning channel # 61\n",
      "The 0 th iteration gives an error 0.06791207\n",
      "The 100 th iteration gives an error 0.0034691258\n",
      "The 200 th iteration gives an error 0.0027801238\n",
      "The 300 th iteration gives an error 0.0023885062\n",
      "The 400 th iteration gives an error 0.0022361442\n",
      "The 500 th iteration gives an error 0.0020948232\n",
      "The 600 th iteration gives an error 0.0019712613\n",
      "The 700 th iteration gives an error 0.001911993\n",
      "The 800 th iteration gives an error 0.0018034556\n",
      "The 900 th iteration gives an error 0.0018059928\n",
      "Time Cost: 3.649465560913086 s\n",
      "Norm of Error =  0.0017709765\n",
      "learning channel # 62\n",
      "The 0 th iteration gives an error 0.079269156\n",
      "The 100 th iteration gives an error 0.0034404285\n",
      "The 200 th iteration gives an error 0.0025628419\n",
      "The 300 th iteration gives an error 0.0021435139\n",
      "The 400 th iteration gives an error 0.0019023449\n",
      "The 500 th iteration gives an error 0.00178357\n",
      "The 600 th iteration gives an error 0.0016329233\n",
      "The 700 th iteration gives an error 0.0016151352\n",
      "The 800 th iteration gives an error 0.0015045871\n",
      "The 900 th iteration gives an error 0.0014389823\n",
      "Time Cost: 3.6245784759521484 s\n",
      "Norm of Error =  0.0014069031\n",
      "learning channel # 63\n",
      "The 0 th iteration gives an error 0.103792526\n",
      "The 100 th iteration gives an error 0.004969237\n",
      "The 200 th iteration gives an error 0.0035722703\n",
      "The 300 th iteration gives an error 0.0030814612\n",
      "The 400 th iteration gives an error 0.0028430265\n",
      "The 500 th iteration gives an error 0.002719075\n",
      "The 600 th iteration gives an error 0.0025331653\n",
      "The 700 th iteration gives an error 0.0024410388\n",
      "The 800 th iteration gives an error 0.0023343747\n",
      "The 900 th iteration gives an error 0.002304414\n",
      "Time Cost: 3.5599138736724854 s\n",
      "Norm of Error =  0.0022451608\n",
      "learning channel # 64\n",
      "The 0 th iteration gives an error 0.11434452\n",
      "The 100 th iteration gives an error 0.0052547846\n",
      "The 200 th iteration gives an error 0.0035027792\n",
      "The 300 th iteration gives an error 0.0030060904\n",
      "The 400 th iteration gives an error 0.0027789045\n",
      "The 500 th iteration gives an error 0.0024703064\n",
      "The 600 th iteration gives an error 0.0023402344\n",
      "The 700 th iteration gives an error 0.002272188\n",
      "The 800 th iteration gives an error 0.00214925\n",
      "The 900 th iteration gives an error 0.0021014132\n",
      "Time Cost: 3.583110809326172 s\n",
      "Norm of Error =  0.0020331806\n",
      "lerning step costs: 4.47770318587621 min\n",
      "Reconstruting Channel # 1\n",
      "Reconstruting Channel # 2\n",
      "Reconstruting Channel # 3\n",
      "Reconstruting Channel # 4\n",
      "Reconstruting Channel # 5\n",
      "Reconstruting Channel # 6\n",
      "Reconstruting Channel # 7\n",
      "Reconstruting Channel # 8\n",
      "Reconstruting Channel # 9\n",
      "Reconstruting Channel # 10\n",
      "Reconstruting Channel # 11\n",
      "Reconstruting Channel # 12\n",
      "Reconstruting Channel # 13\n",
      "Reconstruting Channel # 14\n",
      "Reconstruting Channel # 15\n",
      "Reconstruting Channel # 16\n",
      "Reconstruting Channel # 17\n",
      "Reconstruting Channel # 18\n",
      "Reconstruting Channel # 19\n",
      "Reconstruting Channel # 20\n",
      "Reconstruting Channel # 21\n",
      "Reconstruting Channel # 22\n",
      "Reconstruting Channel # 23\n",
      "Reconstruting Channel # 24\n",
      "Reconstruting Channel # 25\n",
      "Reconstruting Channel # 26\n",
      "Reconstruting Channel # 27\n",
      "Reconstruting Channel # 28\n",
      "Reconstruting Channel # 29\n",
      "Reconstruting Channel # 30\n",
      "Reconstruting Channel # 31\n",
      "Reconstruting Channel # 32\n",
      "Reconstruting Channel # 33\n",
      "Reconstruting Channel # 34\n",
      "Reconstruting Channel # 35\n",
      "Reconstruting Channel # 36\n",
      "Reconstruting Channel # 37\n",
      "Reconstruting Channel # 38\n",
      "Reconstruting Channel # 39\n",
      "Reconstruting Channel # 40\n",
      "Reconstruting Channel # 41\n",
      "Reconstruting Channel # 42\n",
      "Reconstruting Channel # 43\n",
      "Reconstruting Channel # 44\n",
      "Reconstruting Channel # 45\n",
      "Reconstruting Channel # 46\n",
      "Reconstruting Channel # 47\n",
      "Reconstruting Channel # 48\n",
      "Reconstruting Channel # 49\n",
      "Reconstruting Channel # 50\n",
      "Reconstruting Channel # 51\n",
      "Reconstruting Channel # 52\n",
      "Reconstruting Channel # 53\n",
      "Reconstruting Channel # 54\n",
      "Reconstruting Channel # 55\n",
      "Reconstruting Channel # 56\n",
      "Reconstruting Channel # 57\n",
      "Reconstruting Channel # 58\n",
      "Reconstruting Channel # 59\n",
      "Reconstruting Channel # 60\n",
      "Reconstruting Channel # 61\n",
      "Reconstruting Channel # 62\n",
      "Reconstruting Channel # 63\n",
      "Reconstruting Channel # 64\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Training 2/10 || Ry 5 || Acsy 24\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "ACS signal found in the input data\n",
      "go!\n",
      "learning channel # 1\n",
      "The 0 th iteration gives an error 0.12484744\n",
      "The 100 th iteration gives an error 0.0058106463\n",
      "The 200 th iteration gives an error 0.004196281\n",
      "The 300 th iteration gives an error 0.0036157628\n",
      "The 400 th iteration gives an error 0.0032676575\n",
      "The 500 th iteration gives an error 0.0030593402\n",
      "The 600 th iteration gives an error 0.0029044568\n",
      "The 700 th iteration gives an error 0.0027878918\n",
      "The 800 th iteration gives an error 0.0026670357\n",
      "The 900 th iteration gives an error 0.0025923827\n",
      "Time Cost: 107.20957279205322 s\n",
      "Norm of Error =  0.0025553622\n",
      "learning channel # 2\n",
      "The 0 th iteration gives an error 0.12197106\n",
      "The 100 th iteration gives an error 0.005703056\n",
      "The 200 th iteration gives an error 0.004005784\n",
      "The 300 th iteration gives an error 0.003489757\n",
      "The 400 th iteration gives an error 0.0031652069\n",
      "The 500 th iteration gives an error 0.0029554092\n",
      "The 600 th iteration gives an error 0.002750051\n",
      "The 700 th iteration gives an error 0.0027030953\n",
      "The 800 th iteration gives an error 0.0026277746\n",
      "The 900 th iteration gives an error 0.0025367201\n",
      "Time Cost: 3.581603527069092 s\n",
      "Norm of Error =  0.002449687\n",
      "learning channel # 3\n",
      "The 0 th iteration gives an error 0.09697893\n",
      "The 100 th iteration gives an error 0.005315534\n",
      "The 200 th iteration gives an error 0.0040089497\n",
      "The 300 th iteration gives an error 0.0035031596\n",
      "The 400 th iteration gives an error 0.0032280213\n",
      "The 500 th iteration gives an error 0.0030076446\n",
      "The 600 th iteration gives an error 0.002913206\n",
      "The 700 th iteration gives an error 0.0027719853\n",
      "The 800 th iteration gives an error 0.0027198065\n",
      "The 900 th iteration gives an error 0.0026425777\n",
      "Time Cost: 3.687413215637207 s\n",
      "Norm of Error =  0.0025583177\n",
      "learning channel # 4\n",
      "The 0 th iteration gives an error 0.09923945\n",
      "The 100 th iteration gives an error 0.0046549076\n",
      "The 200 th iteration gives an error 0.003377987\n",
      "The 300 th iteration gives an error 0.0028638318\n",
      "The 400 th iteration gives an error 0.0026263348\n",
      "The 500 th iteration gives an error 0.0024300604\n",
      "The 600 th iteration gives an error 0.0023459056\n",
      "The 700 th iteration gives an error 0.0021834974\n",
      "The 800 th iteration gives an error 0.0021020644\n",
      "The 900 th iteration gives an error 0.0019974727\n",
      "Time Cost: 3.6562349796295166 s\n",
      "Norm of Error =  0.0020046807\n",
      "learning channel # 5\n",
      "The 0 th iteration gives an error 0.11623318\n",
      "The 100 th iteration gives an error 0.005949047\n",
      "The 200 th iteration gives an error 0.0042167753\n",
      "The 300 th iteration gives an error 0.0036608125\n",
      "The 400 th iteration gives an error 0.0033422841\n",
      "The 500 th iteration gives an error 0.0031021032\n",
      "The 600 th iteration gives an error 0.0029613366\n",
      "The 700 th iteration gives an error 0.0028518732\n",
      "The 800 th iteration gives an error 0.0029191151\n",
      "The 900 th iteration gives an error 0.0026854083\n",
      "Time Cost: 3.726663827896118 s\n",
      "Norm of Error =  0.002596213\n",
      "learning channel # 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 0 th iteration gives an error 0.10781421\n",
      "The 100 th iteration gives an error 0.005294746\n",
      "The 200 th iteration gives an error 0.003911422\n",
      "The 300 th iteration gives an error 0.0034094146\n",
      "The 400 th iteration gives an error 0.0031438959\n",
      "The 500 th iteration gives an error 0.0029358864\n",
      "The 600 th iteration gives an error 0.002795894\n",
      "The 700 th iteration gives an error 0.0026132152\n",
      "The 800 th iteration gives an error 0.0025739204\n",
      "The 900 th iteration gives an error 0.0024653277\n",
      "Time Cost: 3.720940351486206 s\n",
      "Norm of Error =  0.002443985\n",
      "learning channel # 7\n",
      "The 0 th iteration gives an error 0.09211948\n",
      "The 100 th iteration gives an error 0.004518167\n",
      "The 200 th iteration gives an error 0.0036569545\n",
      "The 300 th iteration gives an error 0.0032835216\n",
      "The 400 th iteration gives an error 0.0029590542\n",
      "The 500 th iteration gives an error 0.0028105816\n",
      "The 600 th iteration gives an error 0.0026464309\n",
      "The 700 th iteration gives an error 0.0026107118\n",
      "The 800 th iteration gives an error 0.0024808953\n",
      "The 900 th iteration gives an error 0.0023921034\n",
      "Time Cost: 3.6871485710144043 s\n",
      "Norm of Error =  0.0023589549\n",
      "learning channel # 8\n",
      "The 0 th iteration gives an error 0.09891915\n",
      "The 100 th iteration gives an error 0.004894545\n",
      "The 200 th iteration gives an error 0.0038891057\n",
      "The 300 th iteration gives an error 0.0034675016\n",
      "The 400 th iteration gives an error 0.0032620267\n",
      "The 500 th iteration gives an error 0.0030587176\n",
      "The 600 th iteration gives an error 0.003003128\n",
      "The 700 th iteration gives an error 0.0028912965\n",
      "The 800 th iteration gives an error 0.0028192801\n",
      "The 900 th iteration gives an error 0.0027578976\n",
      "Time Cost: 3.6931633949279785 s\n",
      "Norm of Error =  0.0026678424\n",
      "learning channel # 9\n",
      "The 0 th iteration gives an error 0.0975927\n",
      "The 100 th iteration gives an error 0.0044308943\n",
      "The 200 th iteration gives an error 0.0033745663\n",
      "The 300 th iteration gives an error 0.0029303478\n",
      "The 400 th iteration gives an error 0.0026169296\n",
      "The 500 th iteration gives an error 0.002454013\n",
      "The 600 th iteration gives an error 0.0023499941\n",
      "The 700 th iteration gives an error 0.002173449\n",
      "The 800 th iteration gives an error 0.0021178059\n",
      "The 900 th iteration gives an error 0.0020731413\n",
      "Time Cost: 3.624129056930542 s\n",
      "Norm of Error =  0.0019948538\n",
      "learning channel # 10\n",
      "The 0 th iteration gives an error 0.10643935\n",
      "The 100 th iteration gives an error 0.004776798\n",
      "The 200 th iteration gives an error 0.0035442715\n",
      "The 300 th iteration gives an error 0.003195668\n",
      "The 400 th iteration gives an error 0.002724503\n",
      "The 500 th iteration gives an error 0.002660046\n",
      "The 600 th iteration gives an error 0.0024661892\n",
      "The 700 th iteration gives an error 0.0023497718\n",
      "The 800 th iteration gives an error 0.0022332235\n",
      "The 900 th iteration gives an error 0.0021642095\n",
      "Time Cost: 3.636815309524536 s\n",
      "Norm of Error =  0.0021300367\n",
      "learning channel # 11\n",
      "The 0 th iteration gives an error 0.12142005\n",
      "The 100 th iteration gives an error 0.006118662\n",
      "The 200 th iteration gives an error 0.0042431187\n",
      "The 300 th iteration gives an error 0.0036325823\n",
      "The 400 th iteration gives an error 0.0033073365\n",
      "The 500 th iteration gives an error 0.0030208684\n",
      "The 600 th iteration gives an error 0.0028902688\n",
      "The 700 th iteration gives an error 0.002687823\n",
      "The 800 th iteration gives an error 0.002581763\n",
      "The 900 th iteration gives an error 0.002556499\n",
      "Time Cost: 3.6934666633605957 s\n",
      "Norm of Error =  0.0025166408\n",
      "learning channel # 12\n",
      "The 0 th iteration gives an error 0.09831154\n",
      "The 100 th iteration gives an error 0.00481001\n",
      "The 200 th iteration gives an error 0.0035317792\n"
     ]
    }
   ],
   "source": [
    "kspace_rraki_recon_all      = np.zeros((M,N,C,len(all_parameters)),dtype = complex)\n",
    "kspace_rraki_acs_recon_all  = np.zeros((M,N,C,len(all_parameters)),dtype = complex)\n",
    "kspace_linear_recon_all     = np.zeros((M,N,C,len(all_parameters)),dtype = complex)\n",
    "kspace_linear_acs_recon_all = np.zeros((M,N,C,len(all_parameters)),dtype = complex)\n",
    "kspace_residual_est_all     = np.zeros((M,N,C,len(all_parameters)),dtype = complex)\n",
    "kspace_raki_truth_all       = np.zeros((M,N,C,len(all_parameters)),dtype = complex)\n",
    "\n",
    "for index,parameter in enumerate(all_parameters):\n",
    "    Ry   = parameter['Ry']\n",
    "    acsy = parameter['acsy']\n",
    "    \n",
    "    print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "    print('Training %d/%d || Ry %d || Acsy %d' % (index+1,len(all_parameters),Ry,acsy))\n",
    "    print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "    \n",
    "    [kspace_rraki,kspace_rraki_acs,kspace_linear,kspace_linear_acs,kspace_residual,kspace_truth] = \\\n",
    "        residual_raki(MaxIteration,LearningRate,Rx,Ry,acsx,acsy,GPU_FRAC)\n",
    "    \n",
    "    kspace_rraki_recon_all[:,:,:,index]      = kspace_rraki\n",
    "    kspace_rraki_acs_recon_all[:,:,:,index]  = kspace_rraki_acs\n",
    "    kspace_linear_recon_all[:,:,:,index]     = kspace_linear\n",
    "    kspace_linear_acs_recon_all[:,:,:,index] = kspace_linear_acs\n",
    "    kspace_residual_est_all[:,:,:,index]     = kspace_residual\n",
    "    kspace_raki_truth_all[:,:,:,index]       = kspace_truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### defining spark helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformattingKspaceForSpark(inputKspace,kspaceOriginal,acsregionX,acsregionY,acsx,acsy,normalizationflag):\n",
    "    [E,C,_,_] = inputKspace.shape\n",
    "    kspaceAcsCrop     = kspaceOriginal[:,:,acsregionX[0]:acsregionX[acsx-1]+1,acsregionY[0]:acsregionY[acsy-1]+1] \n",
    "    #Ground truth measured ACS data, will be used as the ground truth to compute kspace error we want learn\n",
    "    kspaceAcsGrappa   = inputKspace[:,:,acsregionX[0]:acsregionX[acsx-1]+1,acsregionY[0]:acsregionY[acsy-1]+1] \n",
    "    #GRAPPA reconstructed ACS region.  kspaceAcsCrop - kspaceAcsGrappa = d will be the supervised error we try to learn\n",
    "    kspaceAcsDifference = kspaceAcsCrop - kspaceAcsGrappa\n",
    "\n",
    "    #Splitting the difference into the real and imaginary part for the network\n",
    "    acs_difference_real = np.real(kspaceAcsDifference)\n",
    "    acs_difference_imag = np.imag(kspaceAcsDifference)\n",
    "\n",
    "    #print('acs_difference_real shape: ' + str(acs_difference_real.shape))\n",
    "    #print('acs_difference_imag shape: ' + str(acs_difference_imag.shape))\n",
    "\n",
    "    #Adding the batch dimension\n",
    "    kspace_grappa = np.copy(inputKspace)\n",
    "    kspace_grappa_real  = np.real(kspace_grappa)\n",
    "    kspace_grappa_imag  = np.imag(kspace_grappa)\n",
    "    kspace_grappa_split = np.concatenate((kspace_grappa_real, kspace_grappa_imag), axis=1)\n",
    "\n",
    "    #print('kspace_grappa_split shape: ' + str(kspace_grappa_split.shape))\n",
    "\n",
    "    #Let's do some normalization\n",
    "    chan_scale_factors_real = np.zeros((E,C),dtype = 'float')\n",
    "    chan_scale_factors_imag = np.zeros((E,C),dtype = 'float')\n",
    "\n",
    "    for e in range(E):\n",
    "        if(normalizationflag):\n",
    "            scale_factor_input = 1/np.amax(np.abs(kspace_grappa_split[e,:,:,:]))\n",
    "            kspace_grappa_split[e,:,:,:] *= scale_factor_input\n",
    "\n",
    "        for c in range(C):\n",
    "            if(normalizationflag):\n",
    "                scale_factor_real = 1/np.amax(np.abs(acs_difference_real[e,c,:,:]))\n",
    "                scale_factor_imag = 1/np.amax(np.abs(acs_difference_imag[e,c,:,:]))\n",
    "            else:\n",
    "                scale_factor_real = 1\n",
    "                scale_factor_imag = 1\n",
    "\n",
    "            chan_scale_factors_real[e,c] = scale_factor_real\n",
    "            chan_scale_factors_imag[e,c] = scale_factor_imag\n",
    "\n",
    "            acs_difference_real[e,c,:,:] *= scale_factor_real\n",
    "            acs_difference_imag[e,c,:,:] *= scale_factor_imag\n",
    "\n",
    "    acs_difference_real = np.expand_dims(acs_difference_real, axis=2)\n",
    "    acs_difference_real = np.expand_dims(acs_difference_real, axis=2)\n",
    "    acs_difference_imag = np.expand_dims(acs_difference_imag, axis=2)\n",
    "    acs_difference_imag = np.expand_dims(acs_difference_imag, axis=2)\n",
    "\n",
    "    #print('acs_difference_real shape: ' + str(acs_difference_real.shape))\n",
    "    #print('acs_difference_imag shape: ' + str(acs_difference_imag.shape))\n",
    "\n",
    "    kspace_grappa_split = torch.from_numpy(kspace_grappa_split)\n",
    "    kspace_grappa_split = kspace_grappa_split.to(device, dtype=torch.float)\n",
    "    #print('kspace_grappa_split shape: ' + str(kspace_grappa_split.shape))\n",
    "\n",
    "    acs_difference_real = torch.from_numpy(acs_difference_real)\n",
    "    acs_difference_real = acs_difference_real.to(device, dtype=torch.float)\n",
    "    #print('acs_difference_real shape: ' + str(acs_difference_real.shape))\n",
    "\n",
    "    acs_difference_imag = torch.from_numpy(acs_difference_imag)\n",
    "    acs_difference_imag = acs_difference_imag.to(device, dtype=torch.float)\n",
    "    #print('acs_target_imag shape: ' + str(acs_difference_imag.shape))\n",
    "    \n",
    "    return kspace_grappa_split, acs_difference_real, acs_difference_imag, chan_scale_factors_real, chan_scale_factors_imag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingSparkNetwork(kspaceGrappaSplit,acsDifferenceReal,acsDifferenceImag,acsx,acsy,learningRate,iterations):\n",
    "    '''\n",
    "    Trains a SPARK networks given some appropriately formatted grappa kspace, acsDifferenceReal, and acsDifferenceImaginary\n",
    "    Inputs:\n",
    "        kspaceGrappaSplit: allContrasts x 2 * allChannels x M x N,             Grappa reconstructed kspace which will \n",
    "                                                                               be used to learn error\n",
    "        acsDifferenceReal: allContrasts x allChaannels x 1 x 1 x M x N,        Difference between measured and GRAPPA\n",
    "                                                                               ACS real portion\n",
    "        acsDifferenceImag: allContrasts x allChaannels x 1 x 1 M x N,          Difference between measured and GRAPPA\n",
    "                                                                               ACS imag portion             \n",
    "        acs:               acss x 1,                                           Indices of ACS region\n",
    "        learningRate:      scalar,                                             Learaning rate for the networks\n",
    "        iterations:        scalar,                                             Number of iterations we want to train\n",
    "    Outputs:\n",
    "        A network which should reconstruct each contrast and channel        \n",
    "    '''\n",
    "    \n",
    "    [E,C,_,_,_,_] = acsDifferenceReal.shape\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    #Training the real models\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    real_models      = {}\n",
    "    real_model_names = []\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    for e in range(0,E):\n",
    "        for c in range(0,C):\n",
    "            model_name = 'model' + 'E' + str(e) + 'C' + str(c) + 'r'\n",
    "            model = models.SPARK_Netv2(coils = C,kernelsize = 3,acsx = acsx, acsy = acsy)\n",
    "            \n",
    "            model.to(device)\n",
    "            \n",
    "            kspsplit = torch.unsqueeze(kspaceGrappaSplit[e,:,:,:],axis = 0)\n",
    "            \n",
    "            print('Training {}'.format(model_name))\n",
    "            \n",
    "            optimizer = optim.Adam(model.parameters(),lr=learningRate)\n",
    "            running_loss = 0\n",
    "            \n",
    "            for epoch in range(iterations):\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                _,loss_out = model(kspsplit)\n",
    "                loss = criterion(loss_out,acsDifferenceReal[e,c,:,:,:,:])\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                running_loss = loss.item()\n",
    "                if(epoch == 0):\n",
    "                    print('Training started , loss = %.10f' % (running_loss))\n",
    "            \n",
    "            real_model_names.append(model_name)\n",
    "            real_models.update({model_name:model})\n",
    "            \n",
    "            print('Training Complete, loss = %.10f' % (running_loss))\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    #Training the imaginary model\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    imag_models      = {}\n",
    "    imag_model_names = []\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    for e in range(0,E):\n",
    "        for c in range(0,C):\n",
    "            model_name = 'model' + 'E' + str(e) + 'C' + str(c) + 'i'            \n",
    "            model = models.SPARK_Netv2(coils = C,kernelsize = 3,acsx = acsx, acsy = acsy)\n",
    "            \n",
    "            model.to(device)\n",
    "            \n",
    "            kspsplit = torch.unsqueeze(kspaceGrappaSplit[e,:,:,:],axis = 0)\n",
    "            \n",
    "            print('Training {}'.format(model_name))\n",
    "            \n",
    "            optimizer    = optim.Adam(model.parameters(),lr = learningRate)\n",
    "            running_loss = 0.0\n",
    "            \n",
    "            for epoch in range(iterations):\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                _,loss_out = model(kspsplit)\n",
    "                loss = criterion(loss_out,acsDifferenceImag[e,c,:,:,:,:])\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                running_loss = loss.item()\n",
    "                if(epoch == 0):\n",
    "                    print('Training started , loss = %.10f' % (running_loss))\n",
    "                \n",
    "            imag_model_names.append(model_name)\n",
    "            imag_models.update({model_name : model})\n",
    "\n",
    "            print('Training Complete, loss = %.10f' % (running_loss))\n",
    "\n",
    "    return real_models,real_model_names,imag_models,imag_model_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applySparkCorrection(kspaceToCorrect,kspaceGrappaSplit,real_model,imag_model,chanScaleFactorReal,chanScaleFactorImag):\n",
    "    '''\n",
    "    Given a set of models trained for a particular contrast, apply SPARK to all of the contrasts\n",
    "    Inputs:\n",
    "        kspaceToCorrect   - M x N,       Kspace that we want to correct\n",
    "        kspaceGrappasplit - allcoils x M x N  Kspace that will be used to reconstuct the particular for this kspace\n",
    "        real_model      - model          Model for correcting the real component\n",
    "        imag_model      - model          Model for correcting the imaginary component\n",
    "        chanScaleFactor - Scalar         Scaling parameter for the particular piece of kspace which is corrected\n",
    "    outputs:\n",
    "        kspaceCorrected - M x N       Corrected kspace\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    correctionr = real_model(torch.unsqueeze(kspaceGrappaSplit,axis=0))[0].cpu().detach().numpy()\n",
    "    correctioni = imag_model(torch.unsqueeze(kspaceGrappaSplit,axis=0))[0].cpu().detach().numpy()\n",
    "    corrected = correctionr[0,0,:,:]/chanScaleFactorReal + 1j * correctioni[0,0,:,:] / chanScaleFactorImag + kspaceToCorrect\n",
    "    \n",
    "    return corrected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### defining non-changing parameters for SPARK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E = 1\n",
    "normalizationflag = 1\n",
    "normalizeAll      = 0\n",
    "iterations        = 200\n",
    "learningRate      = .0075"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spark reconstruction loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kspace_spark_all = np.zeros((len(all_parameters),C,M,N),dtype = complex)\n",
    "\n",
    "for index,parameter in enumerate(all_parameters):\n",
    "    Ry   = parameter['Ry']\n",
    "    acsy = parameter['acsy']\n",
    "    \n",
    "    print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "    print('Training %d/%d || Ry %d || Acsy %d' % (index+1,len(all_parameters),Ry,acsy))\n",
    "    print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "    \n",
    "    ### Defining k-space to be corrected in SPARK framework ###\n",
    "    kspaceGrappa = np.transpose(np.expand_dims(kspace_rraki_recon_all[:,:,:,index],axis = 0),(0,3,1,2))\n",
    "    kspace       = np.transpose(np.expand_dims(kspace_raki_truth_all[:,:,:,index],axis = 0),(0,3,1,2))\n",
    "    \n",
    "    ### Defining acs region in SPARK framework ###\n",
    "    acsregionX = np.arange(M//2 - acsx // 2,M//2 + acsx//2) \n",
    "    acsregionY = np.arange(N//2 - acsy // 2,N//2 + acsy//2) \n",
    "\n",
    "    kspaceAcsZerofilled = np.zeros((E,C,M,N),dtype = complex)\n",
    "    kspaceAcsZerofilled[:,:,acsregionX[0]:acsregionX[acsx-1]+1,acsregionY[0]:acsregionY[acsy-1]+1] = kspace[:,:,acsregionX[0]:acsregionX[acsx-1]+1,acsregionY[0]:acsregionY[acsy-1]+1]\n",
    "    \n",
    "    ### Training Network ###\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    [kspace_grappa_split, acs_difference_real, acs_difference_imag,chan_scale_factors_real,chan_scale_factors_imag] = \\\n",
    "        reformattingKspaceForSpark(kspaceGrappa,kspaceAcsZerofilled,acsregionX,acsregionY,acsx,acsy,normalizationflag)\n",
    "\n",
    "    realSparkGrappaModels,realSparkGrappaNames,imagSparkGrappaModels,imagSparkGrappaNames = \\\n",
    "        trainingSparkNetwork(kspace_grappa_split,acs_difference_real,acs_difference_imag,acsregionX,acsregionY,learningRate,iterations)\n",
    "\n",
    "    ### Applying SPARK correction with ACS replacement ###\n",
    "    kspaceCorrected    = np.zeros((E,C,M,N),dtype = complex)\n",
    "\n",
    "    for reconContrast in range(0,E):\n",
    "        for c in range(0,C):\n",
    "            #Perform reconstruction coil by coil\n",
    "            model_namer = 'model' + 'E' + str(reconContrast) + 'C' + str(c) + 'r'\n",
    "            model_namei = 'model' + 'E' + str(reconContrast) + 'C' + str(c) + 'i'\n",
    "\n",
    "            real_model = realSparkGrappaModels[model_namer]\n",
    "            imag_model = imagSparkGrappaModels[model_namei]\n",
    "\n",
    "            kspaceToCorrect   = kspaceGrappa[reconContrast,c,:,:]\n",
    "            kspaceGrappaSplit = kspace_grappa_split[reconContrast,:,:,:]\n",
    "\n",
    "            currentCorrected = \\\n",
    "                    applySparkCorrection(kspaceToCorrect,kspaceGrappaSplit,real_model,imag_model,\\\n",
    "                        chan_scale_factors_real[reconContrast,c], chan_scale_factors_imag[reconContrast,c])\n",
    "\n",
    "            kspaceCorrected[reconContrast,c,:,:] = currentCorrected  \n",
    "\n",
    "    kspaceCorrectedReplaced    = np.copy(kspaceCorrected)\n",
    "    kspaceCorrectedReplaced[:,:,acsregionX[0]:acsregionX[acsx-1],acsregionY[0]:acsregionY[acsy-1]] = \\\n",
    "        kspace[:,:,acsregionX[0]:acsregionX[acsx-1],acsregionY[0]:acsregionY[acsy-1]] \n",
    "    \n",
    "    kspace_spark_all[index,:,:,:] = kspaceCorrectedReplaced[0,:,:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing rraki and spark reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rraki_acs_recons = sig.rsos(ifft2c_raki(kspace_rraki_acs_recon_all),2)\n",
    "rraki_recons     = sig.rsos(ifft2c_raki(kspace_rraki_recon_all),2)\n",
    "spark_recons     = np.transpose(sig.rsos(sig.ifft2c(kspace_spark_all),-3),(1,2,0))\n",
    "\n",
    "truth = sig.rsos(sig.ifft2c(kspace),-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate a particular parameter set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = 1\n",
    "\n",
    "print('Ry: %d || Acs y: %d' % (all_parameters[pp]['Ry'],all_parameters[pp]['acsy']))\n",
    "Ry   = parameter['Ry']\n",
    "acsy = parameter['acsy']\n",
    "\n",
    "print('RMSE:')\n",
    "print('   rraki no acs: %.2f' % (sig.rmse(np.squeeze(truth),np.squeeze(rraki_recons[:,:,pp]))*100))\n",
    "print('   rraki w/ acs: %.2f' % (sig.rmse(np.squeeze(truth),np.squeeze(rraki_acs_recons[:,:,pp]))*100))\n",
    "print('   rraki spark:  %.2f' % (sig.rmse(np.squeeze(truth),np.squeeze(spark_recons[:,:,pp]))*100))\n",
    "\n",
    "display = np.concatenate((np.expand_dims(rraki_acs_recons[:,:,pp],axis=0),\\\n",
    "                         np.expand_dims(spark_recons[:,:,pp],axis=0)),axis=0)\n",
    "sig.mosaic(sig.nor(display),1,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {'truth':          np.squeeze(truth),\n",
    "           'all_rraki' :     np.squeeze(rraki_recons),\n",
    "           'all_rraki_acs':  np.squeeze(rraki_acs_recons),\n",
    "           'all_spark':      np.squeeze(spark_recons),\n",
    "           'acs_sizes':      np.squeeze(all_acsy),\n",
    "           'accelerations':  np.squeeze(all_Ry),\n",
    "           'all_parameters': np.squeeze(all_parameters)}\n",
    "\n",
    "sio.savemat('results/residual_raki_with_spark_ablation.mat', results, oned_as='row')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python (tf-gpu)",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
