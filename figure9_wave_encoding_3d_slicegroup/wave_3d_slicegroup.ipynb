{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acute-investment",
   "metadata": {},
   "source": [
    "In this script, I want to perform slice-group SPARK correction by applying SPARK to the collapsed k-space, instead of each individual slice k-space.  By doing this, we don't have to assume that we have a fully sampled ACS region for each slice; instead, we just have to assume that we have our usual integrated ACS region in Ky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alien-imagination",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import warnings\n",
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "import scipy as sp\n",
    "import cupy as cp\n",
    "from bart import bart\n",
    "from utils import cfl\n",
    "from utils import signalprocessing as sig\n",
    "from utils import models\n",
    "from utils import iterative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpha-latest",
   "metadata": {},
   "source": [
    "# Loading dataset and selecting slices/psf to alias "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entire-criticism",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-Load Fx^H * Fy^H * Fz^H * wave_data\n",
    "img_yz = np.transpose(cfl.readcfl('data/img_yz'),(3,2,0,1))\n",
    "\n",
    "#-Load y/z wave psf's\n",
    "PsfY_fit = np.expand_dims(np.expand_dims(cfl.readcfl('data/PsfY_fit'),0),0)\n",
    "PsfZ_fit = np.expand_dims(np.expand_dims(cfl.readcfl('data/PsfZ_fit'),0),0)\n",
    "PsfZ_fit = np.transpose(PsfZ_fit,(0,3,2,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automated-manual",
   "metadata": {},
   "outputs": [],
   "source": [
    "[C,P,Nro,N] = img_yz.shape\n",
    "\n",
    "#-Parameters for the slice-group \n",
    "beginningSliceIndex = 10\n",
    "numslices_all       = 4\n",
    "slicedistance       = P // numslices_all\n",
    "fovshift            = 3 #FOV shift factor for 'caipi' sampling\n",
    "\n",
    "#-acquisition parameters \n",
    "Ry                  = 5   #In-plane acceleration\n",
    "os                  = 3   #How much wave encoding was oversampled by\n",
    "\n",
    "#-Iterative method parameters\n",
    "senseIterations = 20\n",
    "cudaflag        = 1\n",
    "\n",
    "learningRate      = .0075\n",
    "sparkIterations   = 200\n",
    "normalizationflag = 1\n",
    "\n",
    "slices_all = np.linspace(beginningSliceIndex,beginningSliceIndex + slicedistance * (numslices_all-1),numslices_all).astype(int)\n",
    "slices     = slices_all[1::] #Remove frist empty slice\n",
    "numslices  = numslices_all - 1\n",
    "\n",
    "#-Some SPARK parameters\n",
    "acsx = Nro #Acs size in readout dimension\n",
    "acsy = 30  #Acs size in phase encode dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appropriate-purple",
   "metadata": {},
   "source": [
    "## Select slices and associated psf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conceptual-assumption",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_yz_slices = img_yz[:,slices,:,:]\n",
    "psf_slices    = PsfY_fit * PsfZ_fit[:,slices,:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developmental-seller",
   "metadata": {},
   "source": [
    "## Visualize slices to be aliased "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "revolutionary-editing",
   "metadata": {},
   "outputs": [],
   "source": [
    "slices_to_alias_coils = sig.ifft(np.conj(psf_slices) * sig.fft(img_yz_slices,-2),-2)\n",
    "slices_to_alias = sig.rsos(slices_to_alias_coils,-4)\n",
    "sig.mosaic(sig.nor(slices_to_alias),1,numslices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outer-perth",
   "metadata": {},
   "source": [
    "# Generate Cartesian k-space and coil profiles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unexpected-alexander",
   "metadata": {},
   "outputs": [],
   "source": [
    "kspace_slices_cartesian = sig.fft2c(slices_to_alias_coils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "herbal-kingston",
   "metadata": {},
   "outputs": [],
   "source": [
    "coils_slices = np.zeros((C,numslices,Nro,N),dtype = complex)\n",
    "\n",
    "for ss in range(0,numslices):\n",
    "    print('Callibrating coils for slice %d/%d' % (ss + 1,numslices))\n",
    "    curksp  = np.expand_dims(np.transpose(kspace_slices_cartesian[:,ss,:,:],axes = (1,2,0)),2)\n",
    "    coils_slices[:,ss,:,:] = np.squeeze(np.transpose(bart(1,'ecalib -m 1 -c .5',curksp),(3,2,0,1)))\n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dutch-greece",
   "metadata": {},
   "source": [
    "# FOV-shift of slices and sensitivities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "northern-smell",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-Define the shift amounts\n",
    "if(fovshift > 0):\n",
    "    shifts = np.round(np.linspace(-(numslices / 2 - 1),numslices/2,numslices) * N / fovshift ).astype(int)\n",
    "else:\n",
    "    shifts = np.zeros((numslices)).astype(int)\n",
    "\n",
    "#-Define the function which performs the shifting\n",
    "def performshift(x,shift,direction = 1):\n",
    "    out = np.zeros(x.shape,dtype=complex)\n",
    "    \n",
    "    for ss in range(0,out.shape[-3]):\n",
    "        out[:,ss,:,:] = np.roll(x[:,ss,:,:],direction*shift[ss])\n",
    "    return out\n",
    "      \n",
    "#-Compute shifted slices in image space (as well as the shifted coils)\n",
    "slicesShiftedCoils  = performshift(slices_to_alias_coils,shifts)\n",
    "coils               = performshift(coils_slices,shifts)\n",
    "img_yz_slices_shift = performshift(img_yz_slices,shifts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "domestic-collaboration",
   "metadata": {},
   "source": [
    "## Visualize shifted slices and associated wave aliasing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulation-scroll",
   "metadata": {},
   "outputs": [],
   "source": [
    "display = np.squeeze(np.concatenate((np.expand_dims(sig.nor(sig.rsos(slicesShiftedCoils,-4)),0),\\\n",
    "                          np.expand_dims(sig.nor(sig.rsos(img_yz_slices_shift,-4)),0)),axis = 1))\n",
    "sig.mosaic(display,1,2*numslices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handmade-inspection",
   "metadata": {},
   "source": [
    "# Generate cartesian slice-group k-space "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "european-tolerance",
   "metadata": {},
   "outputs": [],
   "source": [
    "col = lambda x: np.sum(x,axis = - 3, keepdims = True)\n",
    "exp = lambda x: np.repeat(x,repeats = numslices,axis = -3)\n",
    "acsregionX = np.arange((Nro*numslices)//2 - acsx // 2,(Nro*numslices)//2 + acsx//2) \n",
    "acsregionY = np.arange(N//2 - acsy // 2,N//2 + acsy//2) \n",
    "\n",
    "#-Generate the undersampling mask\n",
    "mask = np.zeros((C,1,Nro,N),dtype = complex)\n",
    "mask[:,:,:,::Ry] = 1\n",
    "\n",
    "maskAcs = np.zeros((C,1,Nro,N),dtype = complex)\n",
    "maskAcs[:,:,:,::Ry]       = 1\n",
    "maskAcs[:,:,:,acsregionY[0]:acsregionY[acsy-1]] = 1\n",
    "\n",
    "kspace    = col(mask * (sig.fft2c(slicesShiftedCoils)))\n",
    "kspaceAcs = col(maskAcs * (sig.fft2c(slicesShiftedCoils)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demonstrated-weight",
   "metadata": {},
   "source": [
    "# Generate slice-group SENSE operators "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proud-kinase",
   "metadata": {},
   "outputs": [],
   "source": [
    "def senseForward(x,maps,mask):\n",
    "    return mask * col(sig.fft2c(maps*x))\n",
    "def senseAdjoint(x,maps,mask):\n",
    "    xp = cp.get_array_module(x)\n",
    "    return xp.sum(xp.conj(maps)*(sig.ifft2c(exp(x))),-4,keepdims = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "according-lending",
   "metadata": {},
   "source": [
    "# Slice-group SENSE reconstruction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "round-interval",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-Compute the adjoint of the kspace data\n",
    "kadj = senseAdjoint(kspace,coils,mask)\n",
    "kadjAcs = senseAdjoint(kspaceAcs,coils,maskAcs)\n",
    "\n",
    "if(cudaflag):\n",
    "    coils   = cp.asarray(coils)\n",
    "    mask    = cp.asarray(mask)\n",
    "    maskAcs = cp.asarray(maskAcs)\n",
    "    kadj    = cp.asarray(kadj)\n",
    "    kadjAcs = cp.asarray(kadjAcs)\n",
    "    \n",
    "#-Defining the normal operator and performing the reconstruction\n",
    "normal = lambda x: senseAdjoint(senseForward(x.reshape(1,numslices,Nro,N),coils,mask),\\\n",
    "                                          coils,mask).ravel()\n",
    "\n",
    "normalAcs = lambda x: senseAdjoint(senseForward(x.reshape(1,numslices,Nro,N),coils,maskAcs),\\\n",
    "                                          coils,maskAcs).ravel()\n",
    "print('SENSE reconstruction ...',end='')\n",
    "smsSense = cp.asnumpy(iterative.conjgrad(normal,kadj.ravel(),kadj.ravel(),\\\n",
    "                                         ite = 20)).reshape(1,numslices,Nro,N)\n",
    "smsSenseAcs = cp.asnumpy(iterative.conjgrad(normalAcs,kadjAcs.ravel(),kadjAcs.ravel(),\\\n",
    "                                         ite = 20)).reshape(1,numslices,Nro,N)\n",
    "\n",
    "print(' Done.')\n",
    "\n",
    "coils = cp.asnumpy(coils)\n",
    "mask  = cp.asnumpy(mask)\n",
    "kadj  = cp.asnumpy(kadj)\n",
    "kadjAcs = cp.asnumpy(kadjAcs)\n",
    "maskAcs = cp.asnumpy(maskAcs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attached-trinidad",
   "metadata": {},
   "source": [
    "## Evaluate slice-group SENSE reconstruction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compound-forestry",
   "metadata": {},
   "outputs": [],
   "source": [
    "viscrop = lambda x: x[:,768//2-128:768//2+128,:]\n",
    "    \n",
    "truth = viscrop(np.squeeze(performshift(np.expand_dims(np.sum(np.conj(coils) * slicesShiftedCoils,-4),axis = 0),shifts,-1),axis = 0))\n",
    "sense = viscrop(np.squeeze(performshift(np.expand_dims(np.reshape(smsSenseAcs,(numslices,Nro,N)),axis=0),shifts,-1),axis=0))\n",
    "\n",
    "display = sig.nor(np.concatenate((truth,sense),axis = 0))\n",
    "sig.mosaic(display,2,numslices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "criminal-northern",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total rmse:   %.2f' % (sig.rmse(truth,sense)*100) )\n",
    "for ss in range(0,numslices):\n",
    "    print('Slice %d rmse: %.2f' % (ss+1,sig.rmse(truth[ss,:,:],sense[ss,:,:])*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brown-announcement",
   "metadata": {},
   "source": [
    "# Computing k-space for SPARK reconstruction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conservative-brazilian",
   "metadata": {},
   "outputs": [],
   "source": [
    "kspaceSense      = np.transpose(col(sig.fft2c(coils*smsSense)),(1,0,2,3))\n",
    "kspaceAcsSpark   = np.transpose(col((sig.fft2c(slicesShiftedCoils))),(1,0,2,3))\n",
    "acsregionX = np.arange((Nro)//2 - acsx // 2,(Nro)//2 + acsx//2) \n",
    "acsregionY = np.arange(N//2 - acsy // 2,N//2 + acsy//2) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distant-chicken",
   "metadata": {},
   "source": [
    "# Defining SPARK helper functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thick-suite",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformattingKspaceForSpark(inputKspace,kspaceOriginal,acsregionX,acsregionY,acsx,acsy,normalizationflag):\n",
    "    [E,C,_,_] = inputKspace.shape\n",
    "    kspaceAcsCrop     = kspaceOriginal[:,:,acsregionX[0]:acsregionX[acsx-1]+1,acsregionY[0]:acsregionY[acsy-1]+1] \n",
    "    #Ground truth measured ACS data, will be used as the ground truth to compute kspace error we want learn\n",
    "    kspaceAcsGrappa   = inputKspace[:,:,acsregionX[0]:acsregionX[acsx-1]+1,acsregionY[0]:acsregionY[acsy-1]+1] \n",
    "    #GRAPPA reconstructed ACS region.  kspaceAcsCrop - kspaceAcsGrappa = d will be the supervised error we try to learn\n",
    "    kspaceAcsDifference = kspaceAcsCrop - kspaceAcsGrappa\n",
    "\n",
    "    #Splitting the difference into the real and imaginary part for the network\n",
    "    acs_difference_real = np.real(kspaceAcsDifference)\n",
    "    acs_difference_imag = np.imag(kspaceAcsDifference)\n",
    "\n",
    "    #print('acs_difference_real shape: ' + str(acs_difference_real.shape))\n",
    "    #print('acs_difference_imag shape: ' + str(acs_difference_imag.shape))\n",
    "\n",
    "    #Adding the batch dimension\n",
    "    kspace_grappa = np.copy(inputKspace)\n",
    "    kspace_grappa_real  = np.real(kspace_grappa)\n",
    "    kspace_grappa_imag  = np.imag(kspace_grappa)\n",
    "    kspace_grappa_split = np.concatenate((kspace_grappa_real, kspace_grappa_imag), axis=1)\n",
    "\n",
    "    #print('kspace_grappa_split shape: ' + str(kspace_grappa_split.shape))\n",
    "\n",
    "    #Let's do some normalization\n",
    "    chan_scale_factors_real = np.zeros((E,C),dtype = 'float')\n",
    "    chan_scale_factors_imag = np.zeros((E,C),dtype = 'float')\n",
    "\n",
    "    for e in range(E):\n",
    "        if(normalizationflag):\n",
    "            scale_factor_input = 1/np.amax(np.abs(kspace_grappa_split[e,:,:,:]))\n",
    "            kspace_grappa_split[e,:,:,:] *= scale_factor_input\n",
    "\n",
    "        for c in range(C):\n",
    "            if(normalizationflag):\n",
    "                scale_factor_real = 1/np.amax(np.abs(acs_difference_real[e,c,:,:]))\n",
    "                scale_factor_imag = 1/np.amax(np.abs(acs_difference_imag[e,c,:,:]))\n",
    "            else:\n",
    "                scale_factor_real = 1\n",
    "                scale_factor_imag = 1\n",
    "\n",
    "            chan_scale_factors_real[e,c] = scale_factor_real\n",
    "            chan_scale_factors_imag[e,c] = scale_factor_imag\n",
    "\n",
    "            acs_difference_real[e,c,:,:] *= scale_factor_real\n",
    "            acs_difference_imag[e,c,:,:] *= scale_factor_imag\n",
    "\n",
    "    acs_difference_real = np.expand_dims(acs_difference_real, axis=2)\n",
    "    acs_difference_real = np.expand_dims(acs_difference_real, axis=2)\n",
    "    acs_difference_imag = np.expand_dims(acs_difference_imag, axis=2)\n",
    "    acs_difference_imag = np.expand_dims(acs_difference_imag, axis=2)\n",
    "\n",
    "    #print('acs_difference_real shape: ' + str(acs_difference_real.shape))\n",
    "    #print('acs_difference_imag shape: ' + str(acs_difference_imag.shape))\n",
    "\n",
    "    kspace_grappa_split = torch.from_numpy(kspace_grappa_split)\n",
    "    kspace_grappa_split = kspace_grappa_split.to(device, dtype=torch.float)\n",
    "    print('kspace_grappa_split shape: ' + str(kspace_grappa_split.shape))\n",
    "\n",
    "    acs_difference_real = torch.from_numpy(acs_difference_real)\n",
    "    acs_difference_real = acs_difference_real.to(device, dtype=torch.float)\n",
    "    print('acs_difference_real shape: ' + str(acs_difference_real.shape))\n",
    "\n",
    "    acs_difference_imag = torch.from_numpy(acs_difference_imag)\n",
    "    acs_difference_imag = acs_difference_imag.to(device, dtype=torch.float)\n",
    "    print('acs_target_imag shape: ' + str(acs_difference_imag.shape))\n",
    "    \n",
    "    return kspace_grappa_split, acs_difference_real, acs_difference_imag, chan_scale_factors_real, chan_scale_factors_imag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saving-pickup",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingSparkNetwork(kspaceGrappaSplit,acsDifferenceReal,acsDifferenceImag,acsx,acsy,learningRate,iterations):\n",
    "    '''\n",
    "    Trains a SPARK networks given some appropriately formatted grappa kspace, acsDifferenceReal, and acsDifferenceImaginary\n",
    "    Inputs:\n",
    "        kspaceGrappaSplit: allContrasts x 2 * allChannels x M x N,             Grappa reconstructed kspace which will \n",
    "                                                                               be used to learn error\n",
    "        acsDifferenceReal: allContrasts x allChaannels x 1 x 1 x M x N,        Difference between measured and GRAPPA\n",
    "                                                                               ACS real portion\n",
    "        acsDifferenceImag: allContrasts x allChaannels x 1 x 1 M x N,          Difference between measured and GRAPPA\n",
    "                                                                               ACS imag portion             \n",
    "        acs:               acss x 1,                                           Indices of ACS region\n",
    "        learningRate:      scalar,                                             Learaning rate for the networks\n",
    "        iterations:        scalar,                                             Number of iterations we want to train\n",
    "    Outputs:\n",
    "        A network which should reconstruct each contrast and channel        \n",
    "    '''\n",
    "    \n",
    "    [E,C,_,_,_,_] = acsDifferenceReal.shape\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    #Training the real models\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    real_models      = {}\n",
    "    real_model_names = []\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    for e in range(0,E):\n",
    "        for c in range(0,C):\n",
    "            model_name = 'model' + 'E' + str(e) + 'C' + str(c) + 'r'\n",
    "            model = models.SPARK_Netv2(coils = C,kernelsize = 3,acsx = acsx, acsy = acsy)\n",
    "            \n",
    "            model.to(device)\n",
    "            \n",
    "            kspsplit = torch.unsqueeze(kspaceGrappaSplit[e,:,:,:],axis = 0)\n",
    "            \n",
    "            print('Training {}'.format(model_name))\n",
    "            \n",
    "            optimizer = optim.Adam(model.parameters(),lr=learningRate)\n",
    "            running_loss = 0\n",
    "            \n",
    "            for epoch in range(iterations):\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                _,loss_out = model(kspsplit)\n",
    "                loss = criterion(loss_out,acsDifferenceReal[e,c,:,:,:,:])\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                running_loss = loss.item()\n",
    "                if(epoch == 0):\n",
    "                    print('Initial Loss: %.10f' % (running_loss))\n",
    "            \n",
    "            real_model_names.append(model_name)\n",
    "            real_models.update({model_name:model})\n",
    "            print('Final Loss:   %.10f' % (running_loss))\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    #Training the imaginary model\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    imag_models      = {}\n",
    "    imag_model_names = []\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    for e in range(0,E):\n",
    "        for c in range(0,C):\n",
    "            model_name = 'model' + 'E' + str(e) + 'C' + str(c) + 'i'            \n",
    "            model = models.SPARK_Netv2(coils = C,kernelsize = 3,acsx = acsx, acsy = acsy)\n",
    "            \n",
    "            model.to(device)\n",
    "            \n",
    "            kspsplit = torch.unsqueeze(kspaceGrappaSplit[e,:,:,:],axis = 0)\n",
    "            \n",
    "            print('Training {}'.format(model_name))\n",
    "            \n",
    "            optimizer    = optim.Adam(model.parameters(),lr = learningRate)\n",
    "            running_loss = 0.0\n",
    "            \n",
    "            for epoch in range(iterations):\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                _,loss_out = model(kspsplit)\n",
    "                loss = criterion(loss_out,acsDifferenceImag[e,c,:,:,:,:])\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                running_loss = loss.item()\n",
    "                if(epoch == 0):\n",
    "                        print('Initial Loss: %.10f' % (running_loss))\n",
    "                        \n",
    "            imag_model_names.append(model_name)\n",
    "            imag_models.update({model_name : model})\n",
    "\n",
    "            print('Final Loss:   %.10f' % (running_loss))\n",
    "\n",
    "    return real_models,real_model_names,imag_models,imag_model_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outstanding-enzyme",
   "metadata": {},
   "outputs": [],
   "source": [
    "def applySparkCorrection(kspaceToCorrect,kspaceGrappaSplit,real_model,imag_model,chanScaleFactorReal,chanScaleFactorImag):\n",
    "    '''\n",
    "    Given a set of models trained for a particular contrast, apply SPARK to all of the contrasts\n",
    "    Inputs:\n",
    "        kspaceToCorrect   - M x N,       Kspace that we want to correct\n",
    "        kspaceGrappasplit - allcoils x M x N  Kspace that will be used to reconstuct the particular for this kspace\n",
    "        real_model      - model          Model for correcting the real component\n",
    "        imag_model      - model          Model for correcting the imaginary component\n",
    "        chanScaleFactor - Scalar         Scaling parameter for the particular piece of kspace which is corrected\n",
    "    outputs:\n",
    "        kspaceCorrected - M x N       Corrected kspace\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    correctionr = real_model(torch.unsqueeze(kspaceGrappaSplit,axis=0))[0].cpu().detach().numpy()\n",
    "    correctioni = imag_model(torch.unsqueeze(kspaceGrappaSplit,axis=0))[0].cpu().detach().numpy()\n",
    "    corrected = correctionr[0,0,:,:]/chanScaleFactorReal + 1j * correctioni[0,0,:,:] / chanScaleFactorImag + kspaceToCorrect\n",
    "    \n",
    "    return corrected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smaller-forum",
   "metadata": {},
   "source": [
    "# Perform SPARK Training on reconstructed collapsed K-space "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exterior-dependence",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#Reformatting the data\n",
    "[kspace_grappa_split, acs_difference_real, acs_difference_imag,chan_scale_factors_real,chan_scale_factors_imag] = \\\n",
    "    reformattingKspaceForSpark(kspaceSense,kspaceAcsSpark,acsregionX,acsregionY,acsx,acsy,normalizationflag)\n",
    "\n",
    "realSparkGrappaModels,realSparkGrappaNames,imagSparkGrappaModels,imagSparkGrappaNames = \\\n",
    "    trainingSparkNetwork(kspace_grappa_split,acs_difference_real,acs_difference_imag,acsregionX,acsregionY,learningRate,sparkIterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "square-processor",
   "metadata": {},
   "source": [
    "# Perform correction and acs-replacement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valid-monster",
   "metadata": {},
   "outputs": [],
   "source": [
    "#will use each model contrast to reconstruct each recon contrast\n",
    "kspaceCorrected    = np.zeros((1,C,Nro,N),dtype = complex)\n",
    "\n",
    "for e in range(0,1):\n",
    "    for c in range(0,C):\n",
    "        #Perform reconstruction coil by coil\n",
    "        model_namer = 'model' + 'E' + str(e) + 'C' + str(c) + 'r'\n",
    "        model_namei = 'model' + 'E' + str(e) + 'C' + str(c) + 'i'\n",
    "\n",
    "        real_model = realSparkGrappaModels[model_namer]\n",
    "        imag_model = imagSparkGrappaModels[model_namei]\n",
    "\n",
    "        kspaceToCorrect   = kspaceSense[e,c,:,:]\n",
    "        kspaceGrappaSplit = kspace_grappa_split[e,:,:,:]\n",
    "\n",
    "        currentCorrected = \\\n",
    "                applySparkCorrection(kspaceToCorrect,kspaceGrappaSplit,real_model,imag_model,\\\n",
    "                    chan_scale_factors_real[e,c], chan_scale_factors_imag[e,c])\n",
    "\n",
    "        kspaceCorrected[e,c,:,:] = currentCorrected       \n",
    "            \n",
    "#ACS replaced\n",
    "kspaceCorrectedReplaced    = np.copy(kspaceCorrected)\n",
    "\n",
    "\n",
    "kspaceCorrectedReplaced[:,:,acsregionX[0]:acsregionX[acsx-1],acsregionY[0]:acsregionY[acsy-1]] = \\\n",
    "    kspaceAcsSpark[:,:,acsregionX[0]:acsregionX[acsx-1],acsregionY[0]:acsregionY[acsy-1]] \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qualified-investment",
   "metadata": {},
   "source": [
    "# Re-perform cartesian slice-group reconstruction with corrected k-space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closing-transportation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-Compute the adjoint of the kspace data\n",
    "kadj = senseAdjoint(np.transpose(kspaceCorrectedReplaced,(1,0,2,3)),coils,1)\n",
    "\n",
    "if(cudaflag):\n",
    "    coils   = cp.asarray(coils)\n",
    "    kadj    = cp.asarray(kadj)\n",
    "    \n",
    "#-Defining the normal operator and performing the reconstruction\n",
    "normal = lambda x: senseAdjoint(senseForward(x.reshape(1,numslices,Nro,N),coils,1),\\\n",
    "                                          coils,1).ravel()\n",
    "\n",
    "print('SENSE reconstruction ...',end='')\n",
    "smsSenseSpark = cp.asnumpy(iterative.conjgrad(normal,kadj.ravel(),kadj.ravel(),\\\n",
    "                                         ite = 20)).reshape(1,numslices,Nro,N)\n",
    "\n",
    "print(' Done.')\n",
    "\n",
    "coils = cp.asnumpy(coils)\n",
    "kadj  = cp.asnumpy(kadj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liberal-fight",
   "metadata": {},
   "source": [
    "## Visualize Cartesian SENSE with SPARK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ongoing-performance",
   "metadata": {},
   "outputs": [],
   "source": [
    "viscrop = lambda x: x[:,768//2-128:768//2+128,:]\n",
    "    \n",
    "spark = viscrop(np.squeeze(performshift(np.expand_dims(np.reshape(smsSenseSpark,(numslices,Nro,N)),axis=0),shifts,-1),axis=0))\n",
    "\n",
    "display = sig.nor(np.concatenate((truth,sense,spark),axis = 0))\n",
    "sig.mosaic(display,3,numslices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floating-peeing",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Sense Total rmse:   %.2f' % (sig.rmse(truth,sense)*100) )\n",
    "print('Spark Total rmse:   %.2f' % (sig.rmse(truth,spark)*100) )\n",
    "\n",
    "if(numslices > 1):\n",
    "    for ss in range(0,numslices):\n",
    "        print('Slice %d:' %(ss+1))\n",
    "        print('  sense rmse: %.2f' % (sig.rmse(truth[ss,:,:],sense[ss,:,:])*100))\n",
    "        print('  spark rmse: %.2f' % (sig.rmse(truth[ss,:,:],spark[ss,:,:])*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "searching-browse",
   "metadata": {},
   "source": [
    "# Defining wave-encoding operators "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apparent-litigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "psf = np.copy(psf_slices)\n",
    "\n",
    "def sforwave(x,coils):\n",
    "    return coils * x\n",
    "\n",
    "def sadjwave(x,coils):\n",
    "    xp = cp.get_array_module(x)\n",
    "    return xp.sum(xp.conj(coils)*x,-4,keepdims = True)\n",
    "\n",
    "Fx    = lambda x: sig.fft(x,ax = -2)    #Perform fft in the readout direction\n",
    "Fy    = lambda x: sig.fft(x,ax = -1)    #Perform fft in the phaseencode direction\n",
    "Fxadj = lambda x: sig.ifft(x,ax = -2)   #Perform ifft in the readout direction\n",
    "Fyadj = lambda x: sig.ifft(x,ax = -1)   #Perform ifft in the phaseencode direction\n",
    "\n",
    "def waveForward(x,psf): #Perform the forward wave operation through psf modeling\n",
    "    return psf * x\n",
    "\n",
    "def waveAdjoint(x,psf): #Perform the adjoint wave operation through psf modeling\n",
    "    xp = cp.get_array_module(x)\n",
    "    return xp.conj(psf) * x \n",
    "\n",
    "def senseWaveForward(x,maps,psf,mask):\n",
    "    xp = cp.get_array_module(x)\n",
    "    return mask*col(Fy(waveForward(Fx(sforwave(x,maps)),psf)))\n",
    "\n",
    "def senseWaveAdjoint(x,maps,psf,mask):\n",
    "    xp = cp.get_array_module(x)\n",
    "    return sadjwave(Fxadj(waveAdjoint(Fyadj(exp(xp.conj(mask) * x)),psf)),maps)\n",
    "\n",
    "def analyzePsf(x,psf):    \n",
    "    return Fxadj(psf*Fx(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "established-curve",
   "metadata": {},
   "source": [
    "# Generate wave-encoded k-space "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "activated-filename",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-Generate the undersampling mask\n",
    "maskWave = np.zeros((C,1,Nro,N),dtype = complex)\n",
    "maskWave[:,:,:,::Ry] = 1\n",
    "\n",
    "maskWaveAcs = np.zeros((C,1,Nro,N),dtype = complex)\n",
    "maskWaveAcs[:,:,:,::Ry] = 1\n",
    "maskWaveAcs[:,:,:,acsregionY[0]:acsregionY[acsy-1]] = 1\n",
    "\n",
    "kspaceWave = col(maskWave * (sig.fft(psf*sig.fft(slicesShiftedCoils,-2),-1)))\n",
    "kspaceWaveAcs = col(maskWaveAcs * (sig.fft(psf*sig.fft(slicesShiftedCoils,-2),-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surprised-mechanism",
   "metadata": {},
   "source": [
    "# Perform wave-encoded reconstructions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bizarre-memorial",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-Compute the adjoint of the kspace data\n",
    "kadjWave = senseWaveAdjoint(kspaceWave,coils,psf,maskWave)\n",
    "kadjWaveAcs = senseWaveAdjoint(kspaceWaveAcs,coils,psf,maskWaveAcs)\n",
    "\n",
    "if(cudaflag):\n",
    "    coils       = cp.asarray(coils)\n",
    "    maskWave    = cp.asarray(maskWave)\n",
    "    maskWaveAcs = cp.asarray(maskWaveAcs)\n",
    "    kadjWave    = cp.asarray(kadjWave)\n",
    "    kadjWaveAcs = cp.asarray(kadjWaveAcs)\n",
    "    psf         = cp.asarray(psf)\n",
    "    \n",
    "#-Defining the normal operator and performing the reconstruction\n",
    "normalWave = lambda x: senseWaveAdjoint(senseWaveForward(x.reshape(1,numslices,Nro,N),coils,psf,maskWave),\\\n",
    "                                          coils,psf,maskWave).ravel()\n",
    "\n",
    "normalWaveAcs = lambda x: senseWaveAdjoint(senseWaveForward(x.reshape(1,numslices,Nro,N),coils,psf,maskWaveAcs),\\\n",
    "                                          coils,psf,maskWaveAcs).ravel()\n",
    "\n",
    "print('WAVE SENSE reconstruction ...',end='')\n",
    "smsWave = cp.asnumpy(iterative.conjgrad(normalWave,kadjWave.ravel(),kadjWave.ravel(),\\\n",
    "                                         ite = 20)).reshape(1,numslices,Nro,N)\n",
    "\n",
    "smsWaveAcs = cp.asnumpy(iterative.conjgrad(normalWaveAcs,kadjWaveAcs.ravel(),kadjWaveAcs.ravel(),\\\n",
    "                                         ite = 20)).reshape(1,numslices,Nro,N)\n",
    "print(' Done.')\n",
    "\n",
    "coils    = cp.asnumpy(coils)\n",
    "maskWave = cp.asnumpy(maskWave)\n",
    "kadjWave = cp.asnumpy(kadjWave)\n",
    "psf      = cp.asnumpy(psf)\n",
    "maskWaveAcs = cp.asnumpy(maskWaveAcs)\n",
    "kadjWaveAcs = cp.asnumpy(kadjWaveAcs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "announced-liverpool",
   "metadata": {},
   "source": [
    "## Visualizing wave-encoded reconstructions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smart-retail",
   "metadata": {},
   "outputs": [],
   "source": [
    "wave = viscrop(np.squeeze(performshift(np.expand_dims(np.reshape(smsWaveAcs,(numslices,Nro,N)),axis=0),shifts,-1)))\n",
    "\n",
    "display = sig.nor(np.concatenate((truth,sense,wave),axis = 0))\n",
    "sig.mosaic(display,3,numslices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gothic-belgium",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Sense Total rmse:   %.2f' % (sig.rmse(truth,sense)*100) )\n",
    "print('Wave  Total rmse:   %.2f' % (sig.rmse(truth,wave)*100) )\n",
    "\n",
    "for ss in range(0,numslices):\n",
    "    print('Slice %d:' %(ss+1))\n",
    "    print('  sense rmse: %.2f' % (sig.rmse(truth[ss,:,:],sense[ss,:,:])*100))\n",
    "    print('  wave  rmse: %.2f' % (sig.rmse(truth[ss,:,:],wave[ss,:,:])*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "killing-congress",
   "metadata": {},
   "source": [
    "# Setting up k-spaces for wave-encoded SPARK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consistent-partner",
   "metadata": {},
   "outputs": [],
   "source": [
    "kspaceWaveSpark      = np.transpose(col(sig.fft(psf*sig.fft(coils*smsWave,-2),-1)),(1,0,2,3))\n",
    "kspaceWaveAcsSpark   = np.transpose(col(sig.fft(psf*sig.fft(slicesShiftedCoils,-2),-1)),(1,0,2,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parliamentary-beauty",
   "metadata": {},
   "source": [
    "# Training wave-spark network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hairy-assist",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reformatting the data\n",
    "[kspace_grappa_split, acs_difference_real, acs_difference_imag,chan_scale_factors_real,chan_scale_factors_imag] = \\\n",
    "    reformattingKspaceForSpark(kspaceWaveSpark,kspaceWaveAcsSpark,acsregionX,acsregionY,acsx,acsy,normalizationflag)\n",
    "\n",
    "realSparkGrappaModels,realSparkGrappaNames,imagSparkGrappaModels,imagSparkGrappaNames = \\\n",
    "    trainingSparkNetwork(kspace_grappa_split,acs_difference_real,acs_difference_imag,acsregionX,acsregionY,learningRate,sparkIterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "victorian-destination",
   "metadata": {},
   "source": [
    "# Applying SPARAK correction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "biblical-advantage",
   "metadata": {},
   "outputs": [],
   "source": [
    "#will use each model contrast to reconstruct each recon contrast\n",
    "kspaceCorrectedWave    = np.zeros((1,C,Nro,N),dtype = complex)\n",
    "\n",
    "for e in range(0,1):\n",
    "    for c in range(0,C):\n",
    "        #Perform reconstruction coil by coil\n",
    "        model_namer = 'model' + 'E' + str(e) + 'C' + str(c) + 'r'\n",
    "        model_namei = 'model' + 'E' + str(e) + 'C' + str(c) + 'i'\n",
    "\n",
    "        real_model = realSparkGrappaModels[model_namer]\n",
    "        imag_model = imagSparkGrappaModels[model_namei]\n",
    "\n",
    "        kspaceToCorrect   = kspaceWaveSpark[e,c,:,:]\n",
    "        kspaceGrappaSplit = kspace_grappa_split[e,:,:,:]\n",
    "\n",
    "        currentCorrected = \\\n",
    "                applySparkCorrection(kspaceToCorrect,kspaceGrappaSplit,real_model,imag_model,\\\n",
    "                    chan_scale_factors_real[e,c], chan_scale_factors_imag[e,c])\n",
    "\n",
    "        kspaceCorrectedWave[e,c,:,:] = currentCorrected       \n",
    "            \n",
    "#ACS replaced\n",
    "kspaceCorrectedReplacedWave   = np.copy(kspaceCorrectedWave)\n",
    "\n",
    "\n",
    "kspaceCorrectedReplacedWave[:,:,acsregionX[0]:acsregionX[acsx-1],acsregionY[0]:acsregionY[acsy-1]] = \\\n",
    "    kspaceWaveAcsSpark[:,:,acsregionX[0]:acsregionX[acsx-1],acsregionY[0]:acsregionY[acsy-1]] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shaped-pittsburgh",
   "metadata": {},
   "source": [
    "# Performing wave-encoded reconstruction after SPARK correction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perfect-basket",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-Compute the adjoint of the kspace data\n",
    "kadjWave = senseWaveAdjoint(np.transpose(kspaceCorrectedReplacedWave,(1,0,2,3)),coils,psf,1)\n",
    "\n",
    "if(cudaflag):\n",
    "    coils       = cp.asarray(coils)\n",
    "    kadjWave    = cp.asarray(kadjWave)\n",
    "    psf         = cp.asarray(psf)\n",
    "    \n",
    "#-Defining the normal operator and performing the reconstruction\n",
    "normalWave = lambda x: senseWaveAdjoint(senseWaveForward(x.reshape(1,numslices,Nro,N),coils,psf,1),\\\n",
    "                                          coils,psf,1).ravel()\n",
    "\n",
    "print('WAVE SENSE reconstruction ...',end='')\n",
    "smsWaveSpark = cp.asnumpy(iterative.conjgrad(normalWave,kadjWave.ravel(),kadjWave.ravel(),\\\n",
    "                                         ite = 20)).reshape(1,numslices,Nro,N)\n",
    "\n",
    "print(' Done.')\n",
    "\n",
    "coils    = cp.asnumpy(coils)\n",
    "kadjWave = cp.asnumpy(kadjWave)\n",
    "psf      = cp.asnumpy(psf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriented-brunswick",
   "metadata": {},
   "source": [
    "# Displaying results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "institutional-christian",
   "metadata": {},
   "outputs": [],
   "source": [
    "viscrop = lambda x: x[:,768//2-128:768//2+128,:]\n",
    "    \n",
    "sparkWave = viscrop(np.squeeze(performshift(np.expand_dims(np.reshape(smsWaveSpark,(numslices,Nro,N)),axis=0),shifts,-1),axis=0))\n",
    "\n",
    "display = sig.nor(np.concatenate((truth,wave,sparkWave),axis = 0))\n",
    "sig.mosaic(display,3,numslices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worth-syndicate",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('wave  Total rmse:   %.2f' % (sig.rmse(truth,wave)*100) )\n",
    "print('spark Total rmse:   %.2f' % (sig.rmse(truth,sparkWave)*100) )\n",
    "\n",
    "for ss in range(0,numslices):\n",
    "    print('Slice %d:' %(ss+1))\n",
    "    print('  wave  rmse: %.2f' % (sig.rmse(truth[ss,:,:],wave[ss,:,:])*100))\n",
    "    print('  spark rmse: %.2f' % (sig.rmse(truth[ss,:,:],sparkWave[ss,:,:])*100))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
