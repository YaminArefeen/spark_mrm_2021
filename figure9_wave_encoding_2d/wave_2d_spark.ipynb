{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import warnings\n",
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "import scipy as sp\n",
    "import cupy as cp\n",
    "import matplotlib.pyplot as plt\n",
    "from bart import bart\n",
    "from utils import cfl\n",
    "from utils import signalprocessing as sig\n",
    "from utils import models\n",
    "from utils import iterative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### defining spark helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformattingKspaceForSpark(inputKspace,kspaceOriginal,acsregionX,acsregionY,acsx,acsy,normalizationflag):\n",
    "    [E,C,_,_] = inputKspace.shape\n",
    "    kspaceAcsCrop     = kspaceOriginal[:,:,acsregionX[0]:acsregionX[acsx-1]+1,acsregionY[0]:acsregionY[acsy-1]+1] \n",
    "    #Ground truth measured ACS data, will be used as the ground truth to compute kspace error we want learn\n",
    "    kspaceAcsGrappa   = inputKspace[:,:,acsregionX[0]:acsregionX[acsx-1]+1,acsregionY[0]:acsregionY[acsy-1]+1] \n",
    "    #GRAPPA reconstructed ACS region.  kspaceAcsCrop - kspaceAcsGrappa = d will be the supervised error we try to learn\n",
    "    kspaceAcsDifference = kspaceAcsCrop - kspaceAcsGrappa\n",
    "\n",
    "    #Splitting the difference into the real and imaginary part for the network\n",
    "    acs_difference_real = np.real(kspaceAcsDifference)\n",
    "    acs_difference_imag = np.imag(kspaceAcsDifference)\n",
    "\n",
    "    #print('acs_difference_real shape: ' + str(acs_difference_real.shape))\n",
    "    #print('acs_difference_imag shape: ' + str(acs_difference_imag.shape))\n",
    "\n",
    "    #Adding the batch dimension\n",
    "    kspace_grappa = np.copy(inputKspace)\n",
    "    kspace_grappa_real  = np.real(kspace_grappa)\n",
    "    kspace_grappa_imag  = np.imag(kspace_grappa)\n",
    "    kspace_grappa_split = np.concatenate((kspace_grappa_real, kspace_grappa_imag), axis=1)\n",
    "\n",
    "    #print('kspace_grappa_split shape: ' + str(kspace_grappa_split.shape))\n",
    "\n",
    "    #Let's do some normalization\n",
    "    chan_scale_factors_real = np.zeros((E,C),dtype = 'float')\n",
    "    chan_scale_factors_imag = np.zeros((E,C),dtype = 'float')\n",
    "\n",
    "    for e in range(E):\n",
    "        if(normalizationflag):\n",
    "            scale_factor_input = 1/np.amax(np.abs(kspace_grappa_split[e,:,:,:]))\n",
    "            kspace_grappa_split[e,:,:,:] *= scale_factor_input\n",
    "\n",
    "        for c in range(C):\n",
    "            if(normalizationflag):\n",
    "                scale_factor_real = 1/np.amax(np.abs(acs_difference_real[e,c,:,:]))\n",
    "                scale_factor_imag = 1/np.amax(np.abs(acs_difference_imag[e,c,:,:]))\n",
    "            else:\n",
    "                scale_factor_real = 1\n",
    "                scale_factor_imag = 1\n",
    "\n",
    "            chan_scale_factors_real[e,c] = scale_factor_real\n",
    "            chan_scale_factors_imag[e,c] = scale_factor_imag\n",
    "\n",
    "            acs_difference_real[e,c,:,:] *= scale_factor_real\n",
    "            acs_difference_imag[e,c,:,:] *= scale_factor_imag\n",
    "\n",
    "    acs_difference_real = np.expand_dims(acs_difference_real, axis=2)\n",
    "    acs_difference_real = np.expand_dims(acs_difference_real, axis=2)\n",
    "    acs_difference_imag = np.expand_dims(acs_difference_imag, axis=2)\n",
    "    acs_difference_imag = np.expand_dims(acs_difference_imag, axis=2)\n",
    "\n",
    "    #print('acs_difference_real shape: ' + str(acs_difference_real.shape))\n",
    "    #print('acs_difference_imag shape: ' + str(acs_difference_imag.shape))\n",
    "\n",
    "    kspace_grappa_split = torch.from_numpy(kspace_grappa_split)\n",
    "    kspace_grappa_split = kspace_grappa_split.to(device, dtype=torch.float)\n",
    "    print('kspace_grappa_split shape: ' + str(kspace_grappa_split.shape))\n",
    "\n",
    "    acs_difference_real = torch.from_numpy(acs_difference_real)\n",
    "    acs_difference_real = acs_difference_real.to(device, dtype=torch.float)\n",
    "    print('acs_difference_real shape: ' + str(acs_difference_real.shape))\n",
    "\n",
    "    acs_difference_imag = torch.from_numpy(acs_difference_imag)\n",
    "    acs_difference_imag = acs_difference_imag.to(device, dtype=torch.float)\n",
    "    print('acs_target_imag shape: ' + str(acs_difference_imag.shape))\n",
    "    \n",
    "    return kspace_grappa_split, acs_difference_real, acs_difference_imag, chan_scale_factors_real, chan_scale_factors_imag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingSparkNetwork(kspaceGrappaSplit,acsDifferenceReal,acsDifferenceImag,acsx,acsy,learningRate,iterations):\n",
    "    '''\n",
    "    Trains a SPARK networks given some appropriately formatted grappa kspace, acsDifferenceReal, and acsDifferenceImaginary\n",
    "    Inputs:\n",
    "        kspaceGrappaSplit: allContrasts x 2 * allChannels x M x N,             Grappa reconstructed kspace which will \n",
    "                                                                               be used to learn error\n",
    "        acsDifferenceReal: allContrasts x allChaannels x 1 x 1 x M x N,        Difference between measured and GRAPPA\n",
    "                                                                               ACS real portion\n",
    "        acsDifferenceImag: allContrasts x allChaannels x 1 x 1 M x N,          Difference between measured and GRAPPA\n",
    "                                                                               ACS imag portion             \n",
    "        acs:               acss x 1,                                           Indices of ACS region\n",
    "        learningRate:      scalar,                                             Learaning rate for the networks\n",
    "        iterations:        scalar,                                             Number of iterations we want to train\n",
    "    Outputs:\n",
    "        A network which should reconstruct each contrast and channel        \n",
    "    '''\n",
    "    \n",
    "    [E,C,_,_,_,_] = acsDifferenceReal.shape\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    #Training the real models\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    real_models      = {}\n",
    "    real_model_names = []\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    for e in range(0,E):\n",
    "        for c in range(0,C):\n",
    "            model_name = 'model' + 'E' + str(e) + 'C' + str(c) + 'r'\n",
    "            model = models.SPARK_Netv2(coils = C,kernelsize = 3,acsx = acsx, acsy = acsy)\n",
    "            \n",
    "            model.to(device)\n",
    "            \n",
    "            kspsplit = torch.unsqueeze(kspaceGrappaSplit[e,:,:,:],axis = 0)\n",
    "            \n",
    "            print('Training {}'.format(model_name))\n",
    "            \n",
    "            optimizer = optim.Adam(model.parameters(),lr=learningRate)\n",
    "            running_loss = 0\n",
    "            \n",
    "            for epoch in range(iterations):\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                _,loss_out = model(kspsplit)\n",
    "                loss = criterion(loss_out,acsDifferenceReal[e,c,:,:,:,:])\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                running_loss = loss.item()\n",
    "                if(epoch == 0):\n",
    "                    print('Initial Loss: %.10f' % (running_loss))\n",
    "            \n",
    "            real_model_names.append(model_name)\n",
    "            real_models.update({model_name:model})\n",
    "            print('Final Loss:   %.10f' % (running_loss))\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    #Training the imaginary model\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    imag_models      = {}\n",
    "    imag_model_names = []\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    for e in range(0,E):\n",
    "        for c in range(0,C):\n",
    "            model_name = 'model' + 'E' + str(e) + 'C' + str(c) + 'i'            \n",
    "            model = models.SPARK_Netv2(coils = C,kernelsize = 3,acsx = acsx, acsy = acsy)\n",
    "            \n",
    "            model.to(device)\n",
    "            \n",
    "            kspsplit = torch.unsqueeze(kspaceGrappaSplit[e,:,:,:],axis = 0)\n",
    "            \n",
    "            print('Training {}'.format(model_name))\n",
    "            \n",
    "            optimizer    = optim.Adam(model.parameters(),lr = learningRate)\n",
    "            running_loss = 0.0\n",
    "            \n",
    "            for epoch in range(iterations):\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                _,loss_out = model(kspsplit)\n",
    "                loss = criterion(loss_out,acsDifferenceImag[e,c,:,:,:,:])\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                running_loss = loss.item()\n",
    "                if(epoch == 0):\n",
    "                        print('Initial Loss: %.10f' % (running_loss))\n",
    "                        \n",
    "            imag_model_names.append(model_name)\n",
    "            imag_models.update({model_name : model})\n",
    "\n",
    "            print('Final Loss:   %.10f' % (running_loss))\n",
    "\n",
    "    return real_models,real_model_names,imag_models,imag_model_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applySparkCorrection(kspaceToCorrect,kspaceGrappaSplit,real_model,imag_model,chanScaleFactorReal,chanScaleFactorImag):\n",
    "    '''\n",
    "    Given a set of models trained for a particular contrast, apply SPARK to all of the contrasts\n",
    "    Inputs:\n",
    "        kspaceToCorrect   - M x N,       Kspace that we want to correct\n",
    "        kspaceGrappasplit - allcoils x M x N  Kspace that will be used to reconstuct the particular for this kspace\n",
    "        real_model      - model          Model for correcting the real component\n",
    "        imag_model      - model          Model for correcting the imaginary component\n",
    "        chanScaleFactor - Scalar         Scaling parameter for the particular piece of kspace which is corrected\n",
    "    outputs:\n",
    "        kspaceCorrected - M x N       Corrected kspace\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    correctionr = real_model(torch.unsqueeze(kspaceGrappaSplit,axis=0))[0].cpu().detach().numpy()\n",
    "    correctioni = imag_model(torch.unsqueeze(kspaceGrappaSplit,axis=0))[0].cpu().detach().numpy()\n",
    "    corrected = correctionr[0,0,:,:]/chanScaleFactorReal + 1j * correctioni[0,0,:,:] / chanScaleFactorImag + kspaceToCorrect\n",
    "    \n",
    "    return corrected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading dataset and setting parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the fully sampled wave encoded dataset, psf, and coils\n",
    "kspace = np.expand_dims(np.transpose(cfl.readcfl('data/kspaceWaveFull2d'),(2,0,1)),\\\n",
    "                        axis=0)\n",
    "coils  = np.transpose(cfl.readcfl('data/coils2d'),(3,2,0,1))\n",
    "psf    = np.expand_dims(np.expand_dims(cfl.readcfl('data/psf2d'),axis = 0),axis = 0)\n",
    "\n",
    "[nMaps,nCoils,Nro,Npe] = coils.shape\n",
    "\n",
    "#Defining acquisition parameters (acs size and acceleration)\n",
    "Ry   = 6\n",
    "acsx = Nro\n",
    "acsy = 30\n",
    "\n",
    "#-Iterative method parameters\n",
    "senseIterations = 20      #20\n",
    "cudaflag        = 1\n",
    "\n",
    "#-Some SPARK parameters\n",
    "learningRate      = .0075  #.0075\n",
    "sparkIterations   = 200    #200\n",
    "normalizationflag = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generating cartesian k-space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acsregionX = np.arange(Nro//2 - acsx // 2,Nro//2 + acsx//2) \n",
    "acsregionY = np.arange(Npe//2 - acsy // 2,Npe//2 + acsy//2) \n",
    "\n",
    "kspace_cartesian = sig.fft(np.conj(psf)*sig.ifft(kspace,-1),-1)\n",
    "\n",
    "mask = np.zeros((1,nCoils,Nro,Npe),dtype = complex)\n",
    "mask[:,:,:,::Ry] = 1\n",
    "\n",
    "maskAcs = np.zeros((1,nCoils,Nro,Npe),dtype = complex)\n",
    "maskAcs[:,:,:,::Ry] = 1\n",
    "maskAcs[:,:,acsregionX[0]:acsregionX[acsx-1]+1,acsregionY[0]:acsregionY[acsy-1]+1] = 1\n",
    "\n",
    "maskFull = 1\n",
    "\n",
    "kspaceUndersampledCart      = mask * kspace_cartesian\n",
    "kspaceUndersampledAcsCart   = maskAcs * kspace_cartesian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### defining sense operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def senseForward(x,maps,mask):\n",
    "    xp = cp.get_array_module(x)\n",
    "    return mask * sig.fft2c(xp.sum(maps*x,-4,keepdims=True))\n",
    "\n",
    "def senseAdjoint(x,maps,mask):\n",
    "    xp = cp.get_array_module(x)\n",
    "    return xp.sum(xp.conj(maps)*sig.ifft2c(mask*x),-3,keepdims = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### performing sense reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-Compute the adjoint of the kspace data\n",
    "kadj    = senseAdjoint(kspaceUndersampledCart,coils,mask)\n",
    "kadjAcs = senseAdjoint(kspaceUndersampledAcsCart,coils,maskAcs)\n",
    "\n",
    "if(cudaflag):\n",
    "    coils   = cp.asarray(coils)\n",
    "    mask    = cp.asarray(mask)\n",
    "    kadj    = cp.asarray(kadj)\n",
    "    kadjAcs = cp.asarray(kadjAcs)\n",
    "    maskAcs = cp.asarray(maskAcs)\n",
    "    \n",
    "#-Defining the normal operator and performing the reconstruction\n",
    "normal = lambda x: senseAdjoint(senseForward(x.reshape(nMaps,1,Nro,Npe),coils,mask),\\\n",
    "                                          coils,mask).ravel()\n",
    "normalAcs = lambda x: senseAdjoint(senseForward(x.reshape(nMaps,1,Nro,Npe),coils,maskAcs),\\\n",
    "                                          coils,maskAcs).ravel()\n",
    "\n",
    "#print('SENSE reconstruction ...',end='')\n",
    "sense = cp.asnumpy(iterative.conjgrad(normal,kadj.ravel(),kadj.ravel(),\\\n",
    "                                         ite = 20)).reshape(nMaps,1,Nro,Npe)\n",
    "senseAcs = cp.asnumpy(iterative.conjgrad(normalAcs,kadjAcs.ravel(),kadjAcs.ravel(),\\\n",
    "                                         ite = 20)).reshape(nMaps,1,Nro,Npe)\n",
    "print(' Done.')\n",
    "\n",
    "coils = cp.asnumpy(coils)\n",
    "mask  = cp.asnumpy(mask)\n",
    "kadj  = cp.asnumpy(kadj)\n",
    "kadjAcs = cp.asnumpy(kadjAcs)\n",
    "maskAcs = cp.asnumpy(maskAcs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### quick comparisons between sense recons with and without acs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cropreg  = np.arange(Nro//2 - 128,Nro//2 + 128) \n",
    "sensed    = sense[0:1,:,cropreg,:]\n",
    "senseAcsd = senseAcs[0:1,:,cropreg,:]\n",
    "\n",
    "display = sig.nor(np.concatenate((sensed[0,:,:,:],senseAcsd[0,:,:,:]),axis = 0))\n",
    "sig.mosaic(display,1,2,clim = [0,.8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### computing k-space to learn with spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kspaceSenseCart = senseForward(sense,coils,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### performing spark training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "#Reformatting the data\n",
    "[kspace_grappa_split, acs_difference_real, acs_difference_imag,chan_scale_factors_real,chan_scale_factors_imag] = \\\n",
    "    reformattingKspaceForSpark(kspaceSenseCart,kspace_cartesian,acsregionX,acsregionY,acsx,acsy,normalizationflag)\n",
    "\n",
    "realSparkGrappaModels,realSparkGrappaNames,imagSparkGrappaModels,imagSparkGrappaNames = \\\n",
    "    trainingSparkNetwork(kspace_grappa_split,acs_difference_real,acs_difference_imag,acsregionX,acsregionY,learningRate,sparkIterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generating k-space to which we apply correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kspaceSenseToCorrect = senseForward(senseAcs,coils,1)\n",
    "\n",
    "#Reformatting the data\n",
    "[kspace_grappa_split, acs_difference_real, acs_difference_imag,chan_scale_factors_real,chan_scale_factors_imag] = \\\n",
    "    reformattingKspaceForSpark(kspaceSenseToCorrect,kspace_cartesian,acsregionX,acsregionY,acsx,acsy,normalizationflag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### applying correction and acs replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#will use each model contrast to reconstruct each recon contrast\n",
    "kspaceCorrected    = np.zeros((1,nCoils,Nro,Npe),dtype = complex)\n",
    "\n",
    "for c in range(0,nCoils):\n",
    "    #Perform reconstruction coil by coil\n",
    "    model_namer = 'model' + 'E' + str(0) + 'C' + str(c) + 'r'\n",
    "    model_namei = 'model' + 'E' + str(0) + 'C' + str(c) + 'i'\n",
    "\n",
    "    real_model = realSparkGrappaModels[model_namer]\n",
    "    imag_model = imagSparkGrappaModels[model_namei]\n",
    "\n",
    "    kspaceToCorrect   = kspaceSenseToCorrect[0,c,:,:]\n",
    "    kspaceGrappaSplit = kspace_grappa_split[0,:,:,:]\n",
    "\n",
    "    currentCorrected = \\\n",
    "            applySparkCorrection(kspaceToCorrect,kspaceGrappaSplit,real_model,imag_model,\\\n",
    "                chan_scale_factors_real[0,c], chan_scale_factors_imag[0,c])\n",
    "\n",
    "    kspaceCorrected[:,c,:,:] = currentCorrected       \n",
    "            \n",
    "#ACS replaced\n",
    "kspaceCorrectedReplaced    = np.copy(kspaceCorrected)\n",
    "\n",
    "\n",
    "kspaceCorrectedReplaced[:,:,acsregionX[0]:acsregionX[acsx-1],acsregionY[0]:acsregionY[acsy-1]] = \\\n",
    "    kspace_cartesian[:,:,acsregionX[0]:acsregionX[acsx-1],acsregionY[0]:acsregionY[acsy-1]] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### computing sense by resolving sense problem "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-Compute the adjoint of the kspace data\n",
    "kadjSparkCart  = senseAdjoint(kspaceCorrectedReplaced,coils,1)\n",
    "\n",
    "if(cudaflag):\n",
    "    coils           = cp.asarray(coils)\n",
    "    kadjSparkCart   = cp.asarray(kadjSparkCart)\n",
    "    \n",
    "#-Defining the normal operator and performing the reconstruction\n",
    "normalSpark = lambda x: senseAdjoint(senseForward(x.reshape(nMaps,1,Nro,Npe),coils,1),\\\n",
    "                                          coils,1).ravel()\n",
    "\n",
    "print('SENSE reconstructions ...',end='')\n",
    "sparkCart      = cp.asnumpy(iterative.conjgrad(normalSpark,kadjSparkCart.ravel(),kadjSparkCart.ravel(),\\\n",
    "                                         ite = senseIterations)).reshape(nMaps,1,Nro,Npe)\n",
    "print(' Done.')\n",
    "\n",
    "coils       = cp.asnumpy(coils)\n",
    "kadjSpark   = cp.asnumpy(kadjSparkCart)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### quick displaying spark comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkcartd = sparkCart[0:1,:,cropreg,:]\n",
    "\n",
    "display = sig.nor(np.concatenate((senseAcsd[0,:,:,:],sparkcartd[0,:,:,:]),axis = 0))\n",
    "sig.mosaic(display,1,2,clim = [0,.8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### defining wave-encoding operators "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sf(x,coils):\n",
    "    xp = cp.get_array_module(x)\n",
    "    return xp.sum(coils * x,-4,keepdims=True)\n",
    "\n",
    "def Sa(x,coils):\n",
    "    xp = cp.get_array_module(x)\n",
    "    return xp.sum(xp.conj(coils)*x,-3,keepdims = True)\n",
    "\n",
    "Fxf    = lambda x: sig.fft(x,ax = -2)    #Perform fft in the readout direction\n",
    "Fyf    = lambda x: sig.fft(x,ax = -1)    #Perform fft in the phaseencode direction\n",
    "Fxa    = lambda x: sig.ifft(x,ax = -2)   #Perform ifft in the readout direction\n",
    "Fya    = lambda x: sig.ifft(x,ax = -1)   #Perform ifft in the phaseencode direction\n",
    "\n",
    "def Pf(x,psf): #Perform the forward wave operation through psf modeling\n",
    "    return psf * x\n",
    "\n",
    "def Pa(x,psf): #Perform the adjoint wave operation through psf modeling\n",
    "    xp = cp.get_array_module(x)\n",
    "    return xp.conj(psf) * x \n",
    "\n",
    "def senseWaveForward(x,maps,psf,mask):\n",
    "    return mask*Fyf(Pf(Fxf(Sf(x,maps)),psf))\n",
    "\n",
    "def senseWaveAdjoint(x,maps,psf,mask):\n",
    "    return Sa(Fxa(Pa(Fya(mask*x),psf)),maps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generating under-sampled wave-encoded k-space with/without ACS region "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acsregionX = np.arange(Nro//2 - acsx // 2,Nro//2 + acsx//2) \n",
    "acsregionY = np.arange(Npe//2 - acsy // 2,Npe//2 + acsy//2) \n",
    "\n",
    "maskFull = 1\n",
    "\n",
    "#-Generate the undersampling masks with and without acs region\n",
    "maskNoacs = np.zeros((1,1,Nro,Npe))\n",
    "maskNoacs[:,:,:,::Ry] = 1\n",
    "\n",
    "maskAcs = np.zeros((1,1,Nro,Npe))\n",
    "maskAcs[:,:,:,::Ry] = 1\n",
    "maskAcs[:,:,acsregionX[0]:acsregionX[acsx-1]+1,acsregionY[0]:acsregionY[acsy-1]+1] = 1\n",
    "\n",
    "kspaceUndersampled      = maskNoacs * kspace\n",
    "kspaceUndersampledAcs   = maskAcs * kspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### performing wave-encoded reconstructions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-Compute the adjoint of the kspace data\n",
    "kadjFull  = senseWaveAdjoint(kspace,coils,psf,maskFull)\n",
    "kadj      = senseWaveAdjoint(kspaceUndersampled,coils,psf,maskNoacs)\n",
    "kadjAcs   = senseWaveAdjoint(kspaceUndersampledAcs,coils,psf,maskAcs)\n",
    "\n",
    "if(cudaflag):\n",
    "    coils       = cp.asarray(coils)\n",
    "    psf         = cp.asarray(psf)\n",
    "    maskNoacs   = cp.asarray(maskNoacs)\n",
    "    kadj        = cp.asarray(kadj)\n",
    "    maskAcs     = cp.asarray(maskAcs)\n",
    "    kadjAcs     = cp.asarray(kadjAcs)\n",
    "    kadjFull    = cp.asarray(kadjFull)\n",
    "    \n",
    "#-Defining the normal operator and performing the reconstruction\n",
    "normalWaveFull= lambda x: senseWaveAdjoint(senseWaveForward(x.reshape(nMaps,1,Nro,Npe),coils,psf,maskFull),\\\n",
    "                                          coils,psf,maskFull).ravel()\n",
    "normalWave    = lambda x: senseWaveAdjoint(senseWaveForward(x.reshape(nMaps,1,Nro,Npe),coils,psf,maskNoacs),\\\n",
    "                                          coils,psf,maskNoacs).ravel()\n",
    "normalWaveAcs = lambda x: senseWaveAdjoint(senseWaveForward(x.reshape(nMaps,1,Nro,Npe),coils,psf,maskAcs),\\\n",
    "                                          coils,psf,maskAcs).ravel()\n",
    "\n",
    "print('WAVE SENSE reconstructions ...',end='')\n",
    "wave      = cp.asnumpy(iterative.conjgrad(normalWave,kadj.ravel(),kadj.ravel(),\\\n",
    "                                         ite = senseIterations)).reshape(nMaps,1,Nro,Npe)\n",
    "\n",
    "waveAcs   = cp.asnumpy(iterative.conjgrad(normalWaveAcs,kadjAcs.ravel(),kadjAcs.ravel(),\\\n",
    "                                         ite = senseIterations)).reshape(nMaps,1,Nro,Npe)\n",
    "\n",
    "full      = cp.asnumpy(iterative.conjgrad(normalWaveFull,kadjFull.ravel(),kadjFull.ravel(),\\\n",
    "                                         ite = senseIterations)).reshape(nMaps,1,Nro,Npe)\n",
    "print(' Done.')\n",
    "\n",
    "coils       = cp.asnumpy(coils)\n",
    "psf         = cp.asnumpy(psf)\n",
    "maskNoacs   = cp.asnumpy(maskNoacs)\n",
    "kadj        = cp.asnumpy(kadj)\n",
    "maskAcs     = cp.asnumpy(maskAcs)\n",
    "kadjAcs     = cp.asnumpy(kadjAcs)\n",
    "kadjFull    = cp.asnumpy(kadjFull)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### displaying wave-encoded reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cropreg  = np.arange(Nro//2 - 128,Nro//2 + 128) \n",
    "fulld    = full[0:1,:,cropreg,:]\n",
    "waved    = wave[0:1,:,cropreg,:]\n",
    "waveAcsd = waveAcs[0:1,:,cropreg,:]\n",
    "\n",
    "display = sig.nor(np.concatenate((fulld[0,:,:,:],waved[0,:,:,:],waveAcsd[0,:,:,:]),axis = 0))\n",
    "sig.mosaic(display,1,3,clim = [0,.8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### quantifying reconstructions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Wave rmse:         %.2f' % (sig.rmse(fulld,waved)*100))\n",
    "print('Waveacs rmse:      %.2f' % (sig.rmse(fulld,waveAcsd)*100))\n",
    "print('Senseacs rmse:     %.2f' % (sig.rmse(fulld,senseAcsd)*100))\n",
    "print('senseSpark rmse:   %.2f' % (sig.rmse(fulld,sparkcartd)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### computing wave-encoded k-space to learn with SPARK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kspaceSense = senseWaveForward(wave,coils,psf,maskFull)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### performing spark training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "#Reformatting the data\n",
    "[kspace_grappa_split, acs_difference_real, acs_difference_imag,chan_scale_factors_real,chan_scale_factors_imag] = \\\n",
    "    reformattingKspaceForSpark(kspaceSense,kspace,acsregionX,acsregionY,acsx,acsy,normalizationflag)\n",
    "\n",
    "realSparkGrappaModels,realSparkGrappaNames,imagSparkGrappaModels,imagSparkGrappaNames = \\\n",
    "    trainingSparkNetwork(kspace_grappa_split,acs_difference_real,acs_difference_imag,acsregionX,acsregionY,learningRate,sparkIterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generating k-space to which we apply correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kspaceSenseToCorrect = np.copy(kspaceSense) #what I did originally\n",
    "#kspaceSenseToCorrect = senseWaveForward(waveAcs,coils,psf,maskFull)\n",
    "\n",
    "#Reformatting the data\n",
    "[kspace_grappa_split, acs_difference_real, acs_difference_imag,chan_scale_factors_real,chan_scale_factors_imag] = \\\n",
    "    reformattingKspaceForSpark(kspaceSenseToCorrect,kspace,acsregionX,acsregionY,acsx,acsy,normalizationflag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### performing correction with ACS replacement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#will use each model contrast to reconstruct each recon contrast\n",
    "kspaceCorrected    = np.zeros((1,nCoils,Nro,Npe),dtype = complex)\n",
    "\n",
    "for c in range(0,nCoils):\n",
    "    #Perform reconstruction coil by coil\n",
    "    model_namer = 'model' + 'E' + str(0) + 'C' + str(c) + 'r'\n",
    "    model_namei = 'model' + 'E' + str(0) + 'C' + str(c) + 'i'\n",
    "\n",
    "    real_model = realSparkGrappaModels[model_namer]\n",
    "    imag_model = imagSparkGrappaModels[model_namei]\n",
    "\n",
    "    kspaceToCorrect   = kspaceSenseToCorrect[0,c,:,:]\n",
    "    kspaceGrappaSplit = kspace_grappa_split[0,:,:,:]\n",
    "\n",
    "    currentCorrected = \\\n",
    "            applySparkCorrection(kspaceToCorrect,kspaceGrappaSplit,real_model,imag_model,\\\n",
    "                chan_scale_factors_real[0,c], chan_scale_factors_imag[0,c])\n",
    "\n",
    "    kspaceCorrected[:,c,:,:] = currentCorrected       \n",
    "            \n",
    "#ACS replaced\n",
    "kspaceCorrectedReplaced    = np.copy(kspaceCorrected)\n",
    "\n",
    "\n",
    "kspaceCorrectedReplaced[:,:,acsregionX[0]:acsregionX[acsx-1],acsregionY[0]:acsregionY[acsy-1]] = \\\n",
    "    kspace[:,:,acsregionX[0]:acsregionX[acsx-1],acsregionY[0]:acsregionY[acsy-1]] \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### computing final spark reconstruction by resolving sense problem "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-Compute the adjoint of the kspace data\n",
    "kadjSpark  = senseWaveAdjoint(kspaceCorrectedReplaced,coils,psf,maskFull)\n",
    "\n",
    "if(cudaflag):\n",
    "    coils       = cp.asarray(coils)\n",
    "    psf         = cp.asarray(psf)\n",
    "    kadjSpark   = cp.asarray(kadjSpark)\n",
    "    \n",
    "#-Defining the normal operator and performing the reconstruction\n",
    "normalWaveSpark = lambda x: senseWaveAdjoint(senseWaveForward(x.reshape(nMaps,1,Nro,Npe),coils,psf,maskFull),\\\n",
    "                                          coils,psf,maskFull).ravel()\n",
    "\n",
    "print('WAVE SENSE reconstructions ...',end='')\n",
    "spark      = cp.asnumpy(iterative.conjgrad(normalWaveSpark,kadjSpark.ravel(),kadjSpark.ravel(),\\\n",
    "                                         ite = senseIterations)).reshape(nMaps,1,Nro,Npe)\n",
    "print(' Done.')\n",
    "\n",
    "coils       = cp.asnumpy(coils)\n",
    "psf         = cp.asnumpy(psf)\n",
    "kadjSpark   = cp.asnumpy(kadjSpark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### displaying comparisons "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkd = spark[0:1,:,cropreg,:]\n",
    "\n",
    "display = sig.nor(np.concatenate((senseAcsd[0,:,:,:],sparkcartd[0,:,:,:],waveAcsd[0,:,:,:],sparkd[0,:,:,:]),\\\n",
    "                                 axis = 0))\n",
    "sig.mosaic(display,1,4,clim = [0,.8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Sense rmse:      %.2f' % (sig.rmse(fulld,senseAcsd)*100))\n",
    "print('senseSpark rmse: %.2f' % (sig.rmse(fulld,sparkcartd)*100))\n",
    "print('Wave rmse:       %.2f' % (sig.rmse(fulld,waveAcsd)*100))\n",
    "print('waveSpark rmse:  %.2f' % (sig.rmse(fulld,sparkd)*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {'full': np.squeeze(full),\n",
    "           'wave': np.squeeze(waveAcs),\n",
    "           'spark': np.squeeze(spark),\n",
    "           'sense': np.squeeze(senseAcs),\n",
    "           'sensespark' : np.squeeze(sparkCart),\n",
    "           'Ry': Ry,\n",
    "           'acsy': acsy,           \n",
    "           'Iterations': sparkIterations,\n",
    "           'learningRate': learningRate,\n",
    "           'orientation': ori,\n",
    "           'slice': sli}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
